{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13132747,"sourceType":"datasetVersion","datasetId":8319680}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Master Configuration\n**This cell handles all installations, imports, model downloads, and contains a single configuration section for all your settings.**","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n#  1. SETUP: INSTALL AND IMPORT LIBRARIES\n# ==============================================================================\n# For local environments (like Anaconda), uncomment these lines to install libraries.\n# !pip install pandas openpyxl transformers torch tqdm textblob seaborn nltk scikit-learn\n# !pip install huggingface_hub[hf_xet]\n# !pip install --upgrade Pillow==9.5.0\n\n# --- Import Libraries ---\nimport pandas as pd\nimport os\nimport re\nimport string\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport spacy\nfrom transformers import pipeline\nfrom tqdm.auto import tqdm\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import FreqDist, bigrams, trigrams\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom openpyxl.drawing.image import Image\n\n# --- Download NLP Models ---\n# This section ensures all necessary data models are available.\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('vader_lexicon', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('brown', quiet=True) # For TextBlob\n!python -m textblob.download_corpora --quiet\n!python -m spacy download en_core_web_md --quiet\n\n# Register tqdm for use with pandas .progress_apply()\ntqdm.pandas(desc=\"Processing Verbatims\")\n\n# ==============================================================================\n#  2. 🔴 MASTER USER CONFIGURATION 🔴\n# ==============================================================================\n\n# --- Input File Details ---\n# For Kaggle, the path is typically '/kaggle/input/your-dataset-name/your-file-name.xlsx'\nFILE_PATH = r'/kaggle/input/post-training-modification-surveys/Training Surveys - Post-Training.xlsx'\nTEXT_COLUMN_NAME = 'Reason for score'\n\n# --- Output File Details ---\nOUTPUT_FOLDER_PATH = r'/kaggle/working/' # Correct for Kaggle\nPROGRAM_NAME = \"Target\"\nKPIS_IN_SCOPE = \"Training Update\"\nLOBS_IN_SCOPE = \"Post-Modification\"\nMAJOR_VERSION = 1 # Manually change for new datasets or category versions\n\n# --- Analysis Settings ---\nCUSTOM_STOP_WORDS = {'company',str(PROGRAM_NAME),str(KPIS_IN_SCOPE),str(LOBS_IN_SCOPE)}\nTOPIC_MODEL_TOPICS = 5 # Number of topics for LDA analysis\nCLASSIFICATION_THRESHOLD = 0.45 # Confidence score (0.0 to 1.0) for categorization\n\n# --- Define Your Zero-Shot Categories and Sub-Categories (Granular Version) ---\nCATEGORIES = {\n    # --- People Driven Categories ---\n    'Interaction with Call Center Agent': [\n        \"Call agent's communication and listening skills\",\n        \"Call agent's knowledge and problem-solving ability\",\n        \"Call agent's attitude empathy and professionalism\",\n        \"Efficiency and speed of call handling or resolution\",\n    ],\n    'Interaction with In-Store Staff': [\n        \"In-store staff's helpfulness and attitude\",\n        \"Staff's product knowledge and ability to answer questions\",\n        \"Availability and attentiveness of staff in the store\",\n        \"Efficiency of in-store processes like checkout or returns\",\n    ],\n    'Interaction with Field Technician': [\n        \"Technician's professionalism, timeliness, and communication\",\n        \"Technician's skill and ability to fix the issue\",\n        \"Cleanliness and care taken by the technician in the home\",\n        \"Explanation of work performed by the technician\",\n    ],\n\n    # --- Process Driven Category ---\n    'Company Process or Policy Issue': [\n        'Confusion or disagreement with a company policy',\n        'The overall process was too complex or had too many steps',\n        'The total time it took to resolve the issue',\n        'Problems with a follow-up, callback, or promised contact',\n    ],\n\n    # --- Technical and System Categories ---\n    'Website or Online Portal Issue': [\n        \"Website was slow, lagging, or unresponsive\",\n        'Difficulty navigating or finding information on the website',\n        'A website bug, glitch, or error message',\n        'The website crashed, froze, or was unavailable',\n    ],\n    'Mobile Application Issue': [\n        'The mobile app was slow or had poor performance',\n        'A bug or error in the mobile app',\n        'The mobile app crashed or froze',\n        'The mobile app was difficult to use or understand',\n    ],\n    'Communication Channel Quality': [\n        'Poor audio quality, static, or bad phone connection',\n        'Loud background noise during a call',\n        'Issues with the live chat tool or functionality',\n        'Problems with email communication or response times',\n    ],\n\n    # --- Product Driven Category ---\n    'Feedback on the Product Itself': [\n        'The quality, a defect, or damage of the product',\n        'A suggestion or request for a new product feature',\n        'Feedback on the price, cost, or value for money',\n        'The design, appearance, or general ease of use of the product',\n    ]\n}\n\nprint(\"✅ Setup complete. All libraries and models are ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Data Loading, Cleaning, and Sentiment Analysis\n**This cell prepares your core DataFrame by loading, cleaning, and running sentiment analysis.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Load Data Safely ---\n# This block will attempt to load your file. If it fails, it will print an error and stop.\ntry:\n    df = pd.read_excel(FILE_PATH)\n    print(f\"✅ Successfully loaded {len(df)} rows from '{FILE_PATH}'.\")\n    \n    # --- 2. Text Cleaning ---\n    stop_words = set(stopwords.words('english')).union(CUSTOM_STOP_WORDS)\n    lemmatizer = WordNetLemmatizer()\n    def clean_text(text):\n        if not isinstance(text, str): return \"\"\n        text = text.lower()\n        text = re.sub(r'[\\d\\n]', '', text)\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        tokens = word_tokenize(text.strip())\n        cleaned_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]\n        return \" \".join(cleaned_tokens)\n    \n    df.dropna(subset=[TEXT_COLUMN_NAME], inplace=True)\n    df['cleaned_text'] = df[TEXT_COLUMN_NAME].apply(clean_text)\n    \n    # --- 3. Sentiment Analysis ---\n    sia = SentimentIntensityAnalyzer()\n    df['sentiment_compound'] = df[TEXT_COLUMN_NAME].apply(lambda x: sia.polarity_scores(x)['compound'])\n    def categorize_sentiment(compound):\n        if compound >= 0.05: return 'Positive'\n        if compound <= -0.05: return 'Negative'\n        return 'Neutral'\n    df['sentiment_label'] = df['sentiment_compound'].apply(categorize_sentiment)\n\n    print(\"\\n--- Data Preview with Cleaned Text and Sentiment ---\")\n    display(df[[TEXT_COLUMN_NAME, 'cleaned_text', 'sentiment_label']].head())\n\n    # --- 4. Plot Sentiment Distribution and Save ---\n    plt.figure(figsize=(6, 4))\n    df['sentiment_label'].value_counts().plot(kind='bar', color=['green', 'red', 'grey'])\n    plt.title('Sentiment Distribution')\n    plt.ylabel('Number of Responses')\n    plt.xticks(rotation=0)\n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, 'sentiment_distribution.png'), bbox_inches='tight')\n    plt.show()\n\nexcept FileNotFoundError:\n    print(\"=\"*80)\n    print(f\"❌ FATAL ERROR: File not found at the specified path.\")\n    print(f\"   Your specified path: '{FILE_PATH}'\")\n    print(\"   Please check the 'FILE_PATH' variable in your configuration cell and try again.\")\n    print(\"=\"*80)\n    # Raising the error stops the notebook execution\n    raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Exploratory Analysis (Frequency, N-grams, Word Clouds)\n**This cell prepares frequency tables and word cloud images for the final report.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Prepare Overall Text and Tokens ---\nall_cleaned_text = \" \".join(df['cleaned_text'])\nall_tokens = word_tokenize(all_cleaned_text)\n\n# --- 2. Create and Store Frequency Tables ---\nfdist = FreqDist(all_tokens)\ndf_top_words = pd.DataFrame(fdist.most_common(20), columns=['Word', 'Frequency'])\n\nbigram_fdist = FreqDist(list(bigrams(all_tokens)))\ndf_top_bigrams = pd.DataFrame([' '.join(gram) for gram, freq in bigram_fdist.most_common(10)], columns=['Bigram'])\n\ntrigram_fdist = FreqDist(list(trigrams(all_tokens)))\ndf_top_trigrams = pd.DataFrame([' '.join(gram) for gram, freq in trigram_fdist.most_common(10)], columns=['Trigram'])\n\nprint(\"--- Top 20 Most Common Words ---\")\ndisplay(df_top_words)\n\nprint(\"--- Top 10 Most Common Bigrams ---\")\ndisplay(df_top_bigrams)\n\nprint(\"--- Top 10 Most Common Trigrams ---\")\ndisplay(df_top_trigrams)\n\n# --- 3. Generate and Save Word Clouds ---\ndef generate_and_save_wordcloud(text, title, filename):\n    if not text.strip():\n        print(f\"Skipping '{title}' word cloud: No text available.\")\n        return\n    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title)\n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    plt.show()\n\ngenerate_and_save_wordcloud(all_cleaned_text, 'Word Cloud (All Feedback)', 'wordcloud_all.png')\ngenerate_and_save_wordcloud(\" \".join(df[df.sentiment_label == 'Positive']['cleaned_text']), 'Word Cloud (Positive)', 'wordcloud_positive.png')\ngenerate_and_save_wordcloud(\" \".join(df[df.sentiment_label == 'Negative']['cleaned_text']), 'Word Cloud (Negative)', 'wordcloud_negative.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Thematic Analysis (Topic Modeling and Zero-Shot Categorization)\n**This cell performs the main \"what are they talking about?\" analyses.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Topic Modeling (LDA) ---\nprint(\"\\n--- Discovering Latent Topics (LDA) ---\")\nvectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')\ndtm = vectorizer.fit_transform(df['cleaned_text'].dropna())\nif dtm.shape[0] > 1 and dtm.shape[1] > 1:\n    lda = LatentDirichletAllocation(n_components=TOPIC_MODEL_TOPICS, random_state=42)\n    lda.fit(dtm)\n    topic_results = []\n    feature_names = vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(lda.components_):\n        top_words_str = \", \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n        topic_results.append([f\"Topic #{topic_idx + 1}\", top_words_str])\n    df_topics = pd.DataFrame(topic_results, columns=['Discovered Topic', 'Top Words'])\n    display(df_topics)\nelse:\n    print(\"Not enough data to perform topic modeling.\")\n    df_topics = pd.DataFrame() # Create empty df if it fails\n\n# --- 2. Zero-Shot Root Cause Categorization ---\nprint(\"\\n--- Loading Zero-Shot Classification model ---\")\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\ndef get_multi_label_predictions(text, labels, threshold):\n    if not text or not isinstance(text, str): return []\n    results = classifier(text, candidate_labels=labels, multi_label=True)\n    return [label for i, label in enumerate(results['labels']) if results['scores'][i] >= threshold]\n\ndef extract_key_phrases(text):\n    return \"|\".join([str(p) for p in TextBlob(text).noun_phrases[:3]]) if text else \"\"\n\ndef categorize_row(row):\n    text = row[TEXT_COLUMN_NAME]\n    matched_cats = get_multi_label_predictions(text, list(CATEGORIES.keys()), CLASSIFICATION_THRESHOLD)\n    matched_subcats = []\n    if matched_cats:\n        for cat in matched_cats:\n            sub_preds = get_multi_label_predictions(text, CATEGORIES.get(cat, []), CLASSIFICATION_THRESHOLD)\n            matched_subcats.extend(sub_preds)\n        if not matched_subcats and (key_phrases := extract_key_phrases(text)):\n            matched_subcats.append(f\"SUGGESTION: {key_phrases}\")\n    return \"|\".join(matched_cats) if matched_cats else 'Uncategorized', \"|\".join(matched_subcats) if matched_subcats else \"\"\n\nprint(f\"\\n--- Starting Zero-Shot categorization with a threshold of {CLASSIFICATION_THRESHOLD:.2f} ---\")\ndf[['Category', 'Sub-Category']] = df.progress_apply(categorize_row, axis=1, result_type='expand')\n\nprint(\"\\n--- Categorization Complete ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Deep-Dive Categorization Analysis\n**This cell is dedicated to analyzing the results of our categorization, creating the tables and charts needed for the executive summary.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Categorization Summary ---\ntotal_rows = len(df)\nuncategorized_count = len(df[df['Category'] == 'Uncategorized'])\ncategorized_count = total_rows - uncategorized_count\ncategorization_rate = (categorized_count / total_rows) * 100 if total_rows > 0 else 0\nprint(\"\\n--- Categorization Summary ---\")\nsummary_metrics = {\n    'Metric': ['Total Verbatims', 'Categorized', 'Uncategorized', 'Categorization Rate'],\n    'Value': [total_rows, categorized_count, uncategorized_count, f\"{categorization_rate:.2f}%\"]\n}\ndf_summary_metrics = pd.DataFrame(summary_metrics)\ndisplay(df_summary_metrics)\n\n# --- 2.1. Sentiment Breakdown by Category ---\nprint(\"\\n--- Sentiment Breakdown by Category ---\")\ndf_exploded_cat = df.assign(Category=df['Category'].str.split('|')).explode('Category')\ndf_sentiment_by_cat = pd.crosstab(df_exploded_cat['Category'], df_exploded_cat['sentiment_label'])\n\ndf_sentiment_by_cat['Total'] = df_sentiment_by_cat.sum(axis=1)\ndf_sentiment_by_cat = df_sentiment_by_cat.sort_values(by='Total', ascending=False)\ndf_sentiment_by_cat = df_sentiment_by_cat.drop(columns='Total')\n\ndisplay(df_sentiment_by_cat)\n\n# --- 2.2. Sentiment Breakdown by Sub-Category ---\nprint(\"\\n--- Sentiment Breakdown by Sub-Category ---\")\ndf_temp = df.copy()\ndf_temp['Sub-Category'] = df_temp['Sub-Category'].str.split('|')\ndf_exploded_subcat = df_temp.explode('Sub-Category')\ndf_sentiment_by_subcat = pd.crosstab(\n    df_exploded_subcat['Sub-Category'],\n    df_exploded_subcat['sentiment_label']\n)\n\ndf_sentiment_by_subcat['Total'] = df_sentiment_by_subcat.sum(axis=1)\ndf_sentiment_by_subcat = df_sentiment_by_subcat.sort_values(by='Total', ascending=False)\ndf_sentiment_by_subcat = df_sentiment_by_subcat.drop(columns='Total')\n\ndisplay(df_sentiment_by_subcat)\n\n# --- 3. Top Keywords for \"Uncategorized\" Verbatims ---\nprint(\"\\n--- Top Keywords in Uncategorized Verbatims ---\")\nuncategorized_text = \" \".join(df[df['Category'] == 'Uncategorized']['cleaned_text'])\nuncategorized_fdist = FreqDist(word_tokenize(uncategorized_text))\ndf_uncategorized_keywords = pd.DataFrame(uncategorized_fdist.most_common(20), columns=['Keyword', 'Frequency'])\ndisplay(df_uncategorized_keywords.head(10))\n\n# --- 4. Category and Sub-Category Frequency Analysis & Visualization ---\ndef plot_and_save_top_n(series, title, filename, n=15):\n    \"\"\"Helper function to plot and save frequency charts.\"\"\"\n    if series.empty:\n        print(f\"Skipping plot '{title}': No data.\")\n        return\n    plt.figure(figsize=(10, 8))\n    # Plot the top N items in ascending order for the horizontal bar chart\n    series.head(n).sort_values(ascending=True).plot(kind='barh')\n    plt.title(title)\n    plt.xlabel('Count')\n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    plt.show()\n\n# --- Calculate and Plot Overall Frequencies ---\nprint(\"\\n--- Calculating and Plotting Overall Frequencies ---\")\n# Explode categories to get individual counts\ndf_exploded_cat = df.assign(Category=df['Category'].str.split('|')).explode('Category')\ndf_cat_counts = df_exploded_cat['Category'].value_counts()\n\n# Explode sub-categories to get individual counts\ndf_exploded_subcat = df.assign(SubCategory=df['Sub-Category'].str.split('|')).explode('Sub-Category')\ndf_subcat_counts = df_exploded_subcat['Sub-Category'].value_counts()\n\n# Plot the results\nplot_and_save_top_n(df_cat_counts, 'Overall Top Categories', 'freq_cat_overall.png')\nplot_and_save_top_n(df_subcat_counts.drop('', errors='ignore'), 'Overall Top Sub-Categories', 'freq_subcat_overall.png')\n\n\n# --- Calculate and Plot Frequencies Split by Sentiment ---\nprint(\"\\n--- Calculating and Plotting Frequencies by Sentiment ---\")\nfor sentiment in ['Positive', 'Negative', 'Neutral']:\n    # Filter the DataFrame for the current sentiment\n    df_sentiment = df[df['sentiment_label'] == sentiment].copy()\n    if df_sentiment.empty:\n        continue # Skip if no data for this sentiment\n\n    # --- Process and Plot Categories for the sentiment ---\n    df_sentiment['Category'] = df_sentiment['Category'].str.split('|')\n    cat_counts = df_sentiment.explode('Category')['Category'].value_counts()\n    plot_and_save_top_n(cat_counts, f'Top Categories ({sentiment} Sentiment)', f'freq_cat_{sentiment.lower()}.png')\n\n    # --- Process and Plot Sub-Categories for the sentiment ---\n    df_sentiment['Sub-Category'] = df_sentiment['Sub-Category'].str.split('|')\n    subcat_counts = df_sentiment.explode('Sub-Category')['Sub-Category'].value_counts()\n    plot_and_save_top_n(subcat_counts.drop('', errors='ignore'), f'Top Sub-Categories ({sentiment} Sentiment)', f'freq_subcat_{sentiment.lower()}.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Similarity Analysis (Jaccard & Semantic)\n**This cell performs the deeper analysis on vocabulary overlap and semantic relationships.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Jaccard Similarity Between Sentiments ---\ndef get_top_ngrams(tokens, n, N_top):\n    \"\"\"Extracts the top N n-grams from a list of tokens.\"\"\"\n    ngrams_list = tokens if n == 1 else list(nltk.ngrams(tokens, n))\n    return [item for item, freq in FreqDist(ngrams_list).most_common(N_top)]\n\ndef jaccard_similarity(list1, list2):\n    \"\"\"Calculates Jaccard similarity between two lists.\"\"\"\n    set1, set2 = set(list1), set(list2)\n    intersection_len = len(set1.intersection(set2))\n    union_len = len(set1.union(set2))\n    return intersection_len / union_len if union_len > 0 else 0.0\n\n# Calculate Jaccard similarity for top words and bigrams between sentiments\nsentiments = ['Positive', 'Negative', 'Neutral']\ntokens_by_sentiment = {s: word_tokenize(\" \".join(df[df.sentiment_label == s]['cleaned_text'])) for s in sentiments}\ncomparisons = [(\"Positive\", \"Negative\"), (\"Positive\", \"Neutral\"), (\"Negative\", \"Neutral\")]\n\nresults = {\n    \"Comparison\": [f\"{s1} vs. {s2}\" for s1, s2 in comparisons],\n    \"Words (Top 20)\": [jaccard_similarity(get_top_ngrams(tokens_by_sentiment[s1], 1, 20), get_top_ngrams(tokens_by_sentiment[s2], 1, 20)) for s1, s2 in comparisons],\n    \"Bigrams (Top 10)\": [jaccard_similarity(get_top_ngrams(tokens_by_sentiment[s1], 2, 10), get_top_ngrams(tokens_by_sentiment[s2], 2, 10)) for s1, s2 in comparisons],\n    \"Trigrams (Top 10)\": [jaccard_similarity(get_top_ngrams(tokens_by_sentiment[s1], 3, 10), get_top_ngrams(tokens_by_sentiment[s2], 3, 10)) for s1, s2 in comparisons],\n}\nsimilarity_df = pd.DataFrame(results)\n\n# --- 2. Inter-Item Semantic & Jaccard Similarity ---\nnlp = spacy.load(\"en_core_web_md\")\ntop_words_overall = get_top_ngrams(all_tokens, 1, 20) # Top 20 Words\ntop_bigrams_overall = get_top_ngrams(all_tokens, 2, 10) # Top 10 Bigrams\ntop_trigrams_overall = get_top_ngrams(all_tokens, 3, 10) # Top 10 Trigrams\n\n# Word vs Word (Semantic Similarity)\nmatrix_words = np.array([[nlp(w1).similarity(nlp(w2)) for w2 in top_words_overall] for w1 in top_words_overall])\nsimilarity_df_words = pd.DataFrame(matrix_words, index=top_words_overall, columns=top_words_overall)\n\n# Words vs Bigrams (Jaccard Similarity)\nmatrix_words_vs_bigrams = np.array([[jaccard_similarity([word], list(bigram)) for bigram in top_bigrams_overall] for word in top_words_overall])\nsimilarity_df_words_vs_bigrams = pd.DataFrame(matrix_words_vs_bigrams, index=top_words_overall, columns=[' '.join(g) for g in top_bigrams_overall])\n\n# Words vs Trigrams (Jaccard Similarity)\nmatrix_words_vs_trigrams = np.array([[jaccard_similarity([word], list(trigram)) for trigram in top_trigrams_overall] for word in top_words_overall])\nsimilarity_df_words_vs_trigrams = pd.DataFrame(matrix_words_vs_trigrams, index=top_words_overall, columns=[' '.join(g) for g in top_trigrams_overall])\n\n# Bigram vs Bigram (Jaccard Similarity)\nmatrix_bg = np.array([[jaccard_similarity(list(g1), list(g2)) for g2 in top_bigrams_overall] for g1 in top_bigrams_overall])\nsimilarity_df_bigrams = pd.DataFrame(matrix_bg, index=[' '.join(g) for g in top_bigrams_overall], columns=[' '.join(g) for g in top_bigrams_overall])\n\n# Trigram vs Trigram (Jaccard Similarity)\nmatrix_tg = np.array([[jaccard_similarity(list(g1), list(g2)) for g2 in top_trigrams_overall] for g1 in top_trigrams_overall])\nsimilarity_df_trigrams = pd.DataFrame(matrix_tg, index=[' '.join(g) for g in top_trigrams_overall], columns=[' '.join(g) for g in top_trigrams_overall])\n\n\n# --- 3. Generate and Save Heatmap Images ---\ndef create_and_save_heatmap(df_plot, title, filename, annot=False, cmap='viridis', figsize=(12, 10)):\n    \"\"\"Creates, displays, and saves a heatmap from a DataFrame.\"\"\"\n    plt.figure(figsize=figsize)\n    sns.heatmap(df_plot, annot=annot, cmap=cmap, fmt=\".2f\")\n    plt.title(title, fontsize=16)\n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    plt.show()\n\n# Generate and save the heatmaps\ncreate_and_save_heatmap(similarity_df_words, 'Semantic Similarity of Top 20 Words', 'heatmap_words.png')\ncreate_and_save_heatmap(similarity_df_bigrams, 'Jaccard Similarity of Top 10 Bigrams', 'heatmap_bigrams.png', annot=True, cmap='coolwarm')\n\n# ADDED: Heatmap for Trigrams vs Trigrams\ncreate_and_save_heatmap(similarity_df_trigrams, 'Jaccard Similarity of Top 10 Trigrams', 'heatmap_trigrams.png', annot=True, cmap='coolwarm')\n\n# ADDED: Heatmap for Words vs Bigrams\ncreate_and_save_heatmap(similarity_df_words_vs_bigrams, 'Jaccard Similarity: Top Words vs. Top Bigrams', 'heatmap_words_vs_bigrams.png', annot=True, cmap='magma', figsize=(10, 12))\n\n# ADDED: Heatmap for Words vs Trigrams\ncreate_and_save_heatmap(similarity_df_words_vs_trigrams, 'Jaccard Similarity: Top Words vs. Top Trigrams', 'heatmap_words_vs_trigrams.png', annot=True, cmap='magma', figsize=(10, 12))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7: Final Report Generation\n**This final cell gathers every DataFrame and image and compiles them into a single, multi-sheet Excel report.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Generate Versioned Filename ---\ncurrent_date = datetime.now().strftime('%Y-%m-%d')\nbase_filename = f\"{current_date}_{PROGRAM_NAME}_{KPIS_IN_SCOPE}_{LOBS_IN_SCOPE}_Verbatim_Analysis\"\nminor_version = 0\nwhile True:\n    version_str = f\"v{MAJOR_VERSION:02d}.{minor_version:02d}\"\n    output_filename = f\"{base_filename}_{version_str}.xlsx\"\n    full_path = os.path.join(OUTPUT_FOLDER_PATH, output_filename)\n    if not os.path.exists(full_path):\n        break\n    minor_version += 1\n\n# --- 2. Use ExcelWriter to save all results to dedicated sheets ---\nwith pd.ExcelWriter(full_path, engine='openpyxl') as writer:\n    print(f\"\\n--- Writing to Excel file: {output_filename} ---\")\n    \n    # --- Sheet 1: Executive Summary ---\n    # Write key metrics\n    df_summary_metrics.to_excel(writer, sheet_name='Executive_Summary', index=False, startrow=1, startcol=0)\n    \n    # Write overall category counts table, leaving space\n    cat_start_row = len(df_summary_metrics) + 3\n    df_cat_counts.to_frame().reset_index().rename(columns={'index': 'Category', 'Category': 'Count'}).to_excel(\n        writer, sheet_name='Executive_Summary', index=False, startrow=cat_start_row, startcol=0\n    )\n    \n    # Write overall sub-category counts table, leaving space\n    subcat_start_row = cat_start_row + len(df_cat_counts) + 3\n    df_subcat_counts.drop('', errors='ignore').to_frame().reset_index().rename(columns={'index': 'Sub-Category', 'Sub-Category': 'Count'}).to_excel(\n        writer, sheet_name='Executive_Summary', index=False, startrow=subcat_start_row, startcol=0\n    )\n    print(\"Writing Sheet: Executive_Summary\")\n\n    # --- Write Main Data and Analysis Sheets ---\n    df.to_excel(writer, sheet_name='Categorization_Results', index=False)\n    print(\"Writing Sheet: Categorization_Results\")\n    \n    df_sentiment_by_cat.to_excel(writer, sheet_name='Sentiment_by_Category')\n    print(\"Writing Sheet: Sentiment_by_Category\")\n    \n    df_sentiment_by_subcat.to_excel(writer, sheet_name='Sentiment_by_SubCategory')\n    print(\"Writing Sheet: Sentiment_by_SubCategory\")\n\n    if not df_topics.empty:\n        df_topics.to_excel(writer, sheet_name='Topic_Modeling', index=False)\n        print(\"Writing Sheet: Topic_Modeling\")\n\n    if not df_uncategorized_keywords.empty:\n        df_uncategorized_keywords.to_excel(writer, sheet_name='Uncategorized_Keywords', index=False)\n        print(\"Writing Sheet: Uncategorized_Keywords\")\n\n    # --- Write Frequency and Similarity Tables to their own sheets ---\n    df_top_words.to_excel(writer, sheet_name='Top_Words', index=False)\n    print(\"Writing Sheet: Top_Words\")\n    df_top_bigrams.to_excel(writer, sheet_name='Top_Bigrams', index=False)\n    print(\"Writing Sheet: Top_Bigrams\")\n    df_top_trigrams.to_excel(writer, sheet_name='Top_Trigrams', index=False)\n    print(\"Writing Sheet: Top_Trigrams\")\n\n    similarity_df.to_excel(writer, sheet_name='Vocab_Similarity', index=False)\n    print(\"Writing Sheet: Vocab_Similarity\")\n    \n    similarity_df_words.to_excel(writer, sheet_name='Word_Similarity_Matrix')\n    print(\"Writing Sheet: Word_Similarity_Matrix\")\n    \n    similarity_df_bigrams.to_excel(writer, sheet_name='Bigram_Similarity_Matrix')\n    print(\"Writing Sheet: Bigram_Similarity_Matrix\")\n    \n    similarity_df_trigrams.to_excel(writer, sheet_name='Trigram_Similarity_Matrix')\n    print(\"Writing Sheet: Trigram_Similarity_Matrix\")\n    \n    similarity_df_words_vs_bigrams.to_excel(writer, sheet_name='Words_vs_Bigrams_Sim')\n    print(\"Writing Sheet: Words_vs_Bigrams_Sim\")\n    \n    similarity_df_words_vs_trigrams.to_excel(writer, sheet_name='Words_vs_Trigrams_Sim')\n    print(\"Writing Sheet: Words_vs_Trigrams_Sim\")\n    \n    # --- Embed Images into the Excel Sheets ---\n    workbook = writer.book\n    def embed_image(ws_name, img_path, cell_loc):\n        if os.path.exists(img_path):\n            ws = workbook.create_sheet(title=ws_name)\n            img = Image(img_path)\n            ws.add_image(img, cell_loc)\n            print(f\"Embedding Image: {os.path.basename(img_path)} into sheet: {ws_name}\")\n\n    # Embed summary charts into Executive Summary sheet\n    ws_summary = workbook['Executive_Summary']\n    if os.path.exists(os.path.join(OUTPUT_FOLDER_PATH, 'freq_cat_overall.png')):\n        img_cat = Image(os.path.join(OUTPUT_FOLDER_PATH, 'freq_cat_overall.png'))\n        ws_summary.add_image(img_cat, 'D2') # Position next to category table\n    if os.path.exists(os.path.join(OUTPUT_FOLDER_PATH, 'freq_subcat_overall.png')):\n        img_subcat = Image(os.path.join(OUTPUT_FOLDER_PATH, 'freq_subcat_overall.png'))\n        ws_summary.add_image(img_subcat, f'D{cat_start_row + len(df_cat_counts) + 3}') # Position next to sub-cat table\n    \n    # Embed all other charts and images into their own dedicated sheets\n    embed_image('Sentiment_Chart', os.path.join(OUTPUT_FOLDER_PATH, 'sentiment_distribution.png'), 'A1')\n    embed_image('Word_Clouds', os.path.join(OUTPUT_FOLDER_PATH, 'wordcloud_all.png'), 'A1')\n    if os.path.exists(os.path.join(OUTPUT_FOLDER_PATH, 'wordcloud_positive.png')):\n        workbook['Word_Clouds'].add_image(Image(os.path.join(OUTPUT_FOLDER_PATH, 'wordcloud_positive.png')), 'A25')\n    if os.path.exists(os.path.join(OUTPUT_FOLDER_PATH, 'wordcloud_negative.png')):\n        workbook['Word_Clouds'].add_image(Image(os.path.join(OUTPUT_FOLDER_PATH, 'wordcloud_negative.png')), 'A50')\n    \n    embed_image('Heatmap_Word_Sim', os.path.join(OUTPUT_FOLDER_PATH, 'heatmap_words.png'), 'A1')\n    embed_image('Heatmap_Bigram_Sim', os.path.join(OUTPUT_FOLDER_PATH, 'heatmap_bigrams.png'), 'A1')\n    embed_image('Heatmap_Trigram_Sim', os.path.join(OUTPUT_FOLDER_PATH, 'heatmap_trigrams.png'), 'A1')\n    embed_image('Heatmap_Words_v_Bigrams', os.path.join(OUTPUT_FOLDER_PATH, 'heatmap_words_vs_bigrams.png'), 'A1')\n    embed_image('Heatmap_Words_v_Trigrams', os.path.join(OUTPUT_FOLDER_PATH, 'heatmap_words_vs_trigrams.png'), 'A1')\n    \n    embed_image('Charts_Pos_Cats', os.path.join(OUTPUT_FOLDER_PATH, 'freq_cat_positive.png'), 'A1')\n    embed_image('Charts_Neg_Cats', os.path.join(OUTPUT_FOLDER_PATH, 'freq_cat_negative.png'), 'A1')\n    embed_image('Charts_Neu_Cats', os.path.join(OUTPUT_FOLDER_PATH, 'freq_cat_neutral.png'), 'A1')\n    \n    embed_image('Charts_Pos_SubCats', os.path.join(OUTPUT_FOLDER_PATH, 'freq_subcat_positive.png'), 'A1')\n    embed_image('Charts_Neg_SubCats', os.path.join(OUTPUT_FOLDER_PATH, 'freq_subcat_negative.png'), 'A1')\n    embed_image('Charts_Neu_SubCats', os.path.join(OUTPUT_FOLDER_PATH, 'freq_subcat_neutral.png'), 'A1')\n\n\nprint(f\"\\n✅ All analysis results and plots have been saved to a multi-sheet file:\")\nprint(full_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clear the /Kaggle/Working output directory","metadata":{}},{"cell_type":"code","source":"# # Ctrl + / to un/comment out code while highlighted\n\n# import os\n# import shutil\n\n# # This is the directory you want to clear\n# output_dir = '/kaggle/working/'\n\n# # Loop through everything in the directory\n# for filename in os.listdir(output_dir):\n#     file_path = os.path.join(output_dir, filename)\n#     try:\n#         # If it's a file or link, delete it\n#         if os.path.isfile(file_path) or os.path.islink(file_path):\n#             os.unlink(file_path)\n#         # If it's a directory, delete it and all its contents\n#         elif os.path.isdir(file_path):\n#             shutil.rmtree(file_path)\n#         print(f\"Deleted: {filename}\")\n#     except Exception as e:\n#         print(f'Failed to delete {file_path}. Reason: {e}')\n\n# print(\"\\n✅ Output directory has been cleared.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}