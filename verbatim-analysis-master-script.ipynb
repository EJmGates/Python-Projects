{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13282735,"sourceType":"datasetVersion","datasetId":8418058}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Master Configuration\n**This cell handles all installations, imports, model downloads, and contains a single configuration section for all your settings.**","metadata":{}},{"cell_type":"code","source":"# For local environments (like Anaconda), uncomment these lines to install libraries.\n# !pip install pandas openpyxl transformers torch tqdm textblob seaborn nltk scikit-learn\n# !pip install --upgrade transformers huggingface_hub\n# !pip install --upgrade Pillow==9.5.0\n!pip install transformers==4.35.2 accelerate==0.24.1 huggingface_hub==0.19.4\n!pip install langdetect\n!pip install deep-translator\n!pip install ipywidgets\n\n# --- Import Libraries ---\nimport pandas as pd\nimport os\nimport re\nimport string\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport scipy.stats as stats\nimport seaborn as sns\nimport nltk\nimport spacy\nfrom transformers import pipeline\nfrom tqdm.auto import tqdm\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import FreqDist, bigrams, trigrams\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.metrics import silhouette_score\nfrom openpyxl.drawing.image import Image\nfrom openpyxl.styles import Alignment, Border, Side\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.models.ldamodel import LdaModel\n\n# --- Input desired plot style here ---\nplt.style.use('seaborn-v0_8-deep') \n\n# --- Download NLP Models ---\n# This section ensures all necessary data models are available.\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('vader_lexicon', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('brown', quiet=True)\n!python -m textblob.download_corpora --quiet\n!python -m spacy download en_core_web_md --quiet\n\n# Register tqdm for use with pandas .progress_apply()\ntqdm.pandas(desc=\"Processing Verbatims\")\n\n# ==============================================================================\n#  2. ðŸ”´ MASTER USER CONFIGURATION ðŸ”´\n# ==============================================================================\n\n# --- Input File Details ---\n# For Kaggle, the path is typically '/kaggle/input/your-dataset-name/your-file-name.xlsx'\nFILE_PATH = r'/kaggle/input/fanatics-verbatim-jun-aug-2025-02/Fanatics Verbatim Data June-August 2025.xlsx'\nTEXT_COLUMN_NAME = 'Survey Verbatim'\n\n# --- Analysis Details ---\nOUTPUT_FOLDER_PATH = r'/kaggle/working/' # Correct for Kaggle\nPROGRAM_NAME = \"Fanatics\"\nKPIS_IN_SCOPE = \"CSAT\"\nLOBS_IN_SCOPE = \"Voice and Chat\"\nSCORE_COLUMN_NAME = 'CSAT Score'\nMAJOR_VERSION = 1 # Manually change for new datasets or category versions\n\n# --- Analysis Settings ---\nCUSTOM_STOP_WORDS = {\n    # --- Generic Business & Service Terms ---\n    # 'company', 'business', 'service', 'services', 'product', 'products', 'team', 'customer', \n    # 'customers', 'client', 'clients', 'staff', 'agent', 'representative', 'organization',\n    # 'experience', 'issue', 'issues', 'problem', 'problems', 'question', 'questions', \n    # 'feedback', 'inquiry', 'inquiries', 'request', 'requests', 'ticket', 'case', 'account',\n\n    # --- Common Vague Fillers & Conversational Terms ---\n    'also', 'really', 'actually', 'just', 'like', 'im', 'ive', 'thing', 'things', \n    'something', 'anything', 'everything', 'well', 'get', 'got', 'getting', 'would', 'could', \n    'should', 'make', 'made',\n    \n    # 'one', 'even', 'since', 'every', 'time', 'times', 'day', 'days', \n    # 'week', 'weeks', 'month', 'months', 'year', 'years', 'lot', 'always'\n\n    # --- Politeness, Greetings, & Inquiries ---\n    # 'please', 'help', 'hello', 'hi', 'hey', 'thank', 'thanks', 'appreciate', 'regards', 'best', 'since', 'under', 'within',\n    # 'know', 'see', 'want', 'wanted', 'looking', 'wondering', 'information', 'info', 'details', 'beyond', 'over',\n    \n    # --- Placeholders for Your Specific Names ---\n    'company',str(PROGRAM_NAME),str(KPIS_IN_SCOPE),str(LOBS_IN_SCOPE),\n\n    # --- Prepositions ---\n    'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'among', 'around', 'at',\n    'before', 'behind', 'below', 'beneath', 'beside', 'between', 'by', 'down', 'during',\n    'for', 'from', 'in', 'inside', 'into', 'like', 'near', 'of', 'off', 'on', 'onto', 'out',\n    'outside', 'past', 'through', 'throughout', 'to', 'toward',\n    'underneath', 'until', 'up', 'upon', 'with', 'without', 'is', 'a', 'or'\n}\n\nCLASSIFICATION_THRESHOLD = 0.55 # Confidence score (0.0 to 1.0) for categorization\n\n# --- Define Your Zero-Shot Categories and Sub-Categories (Granular Version) ---\nCATEGORIES = {\n    # --- People Driven Categories ---\n    'Interaction with Call Center Agent': [\n        \"Call agent's communication and listening skills\",\n        \"Call agent's knowledge and problem-solving ability\",\n        \"Call agent's attitude empathy and professionalism\",\n        \"Efficiency and speed of call handling or resolution\",\n    ],\n    'Interaction with In-Store Staff': [\n        \"In-store staff's helpfulness and attitude\",\n        \"Staff's product knowledge and ability to answer questions\",\n        \"Availability and attentiveness of staff in the store\",\n        \"Efficiency of in-store processes like checkout or returns\",\n    ],\n    'Interaction with Field Technician': [\n        \"Technician's professionalism, timeliness, and communication\",\n        \"Technician's skill and ability to fix the issue\",\n        \"Cleanliness and care taken by the technician in the home\",\n        \"Explanation of work performed by the technician\",\n    ],\n\n    # --- Process Driven Category ---\n    'Company Process or Policy Issue': [\n        'Confusion or disagreement with a company policy',\n        'The overall process was too complex or had too many steps',\n        'The total time it took to resolve the issue',\n        'Problems with a follow-up, callback, or promised contact',\n    ],\n\n    # --- Technical and System Categories ---\n    'Website or Online Portal Issue': [\n        \"Website was slow, lagging, or unresponsive\",\n        'Difficulty navigating or finding information on the website',\n        'A website bug, glitch, or error message',\n        'The website crashed, froze, or was unavailable',\n    ],\n    'Mobile Application Issue': [\n        'The mobile app was slow or had poor performance',\n        'A bug or error in the mobile app',\n        'The mobile app crashed or froze',\n        'The mobile app was difficult to use or understand',\n    ],\n    'Communication Channel Quality': [\n        'Poor audio quality, static, or bad phone connection',\n        'Loud background noise during a call',\n        'Issues with the live chat tool or functionality',\n        'Problems with email communication or response times',\n    ],\n\n    # --- Product Driven Category ---\n    'Feedback on the Product Itself': [\n        'The quality, a defect, or damage of the product',\n        'A suggestion or request for a new product feature',\n        'Feedback on the price, cost, or value for money',\n        'The design, appearance, or general ease of use of the product',\n    ]\n}\n\nprint(\"âœ… Setup complete. All libraries and models are ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 1.1: Category Selector","metadata":{}},{"cell_type":"code","source":"# Import the ipywidgets library\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Get your list of Level 1 categories from the CATEGORIES dictionary\nall_categories = list(CATEGORIES.keys())\n\n# Create the interactive multi-select widget\ncategory_selector = widgets.SelectMultiple(\n    options=all_categories,\n    value=all_categories, # Pre-selects all categories by default\n    description='Categories:',\n    disabled=False,\n    layout={'width': '50%'} # Adjust width as needed\n)\n\n# Display the widget in the cell's output\nprint(\"Hold Ctrl (or Cmd on Mac) to select multiple categories.\")\ndisplay(category_selector)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Data Loading, Cleaning, and Sentiment Analysis\n**This cell prepares your core DataFrame by loading, cleaning, and running sentiment analysis.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Load Data Safely ---\ntry:\n    df = pd.read_excel(FILE_PATH)\n    print(f\"âœ… Successfully loaded {len(df)} rows from '{FILE_PATH}'.\")\n\n    # --- 2. Text Cleaning (UPGRADED AND FINAL) ---\n    # Import necessary libraries\n    from textblob import TextBlob\n    from langdetect import detect, LangDetectException\n    from deep_translator import GoogleTranslator # âœ¨ USE DEDICATED TRANSLATOR\n\n    stop_words = set(stopwords.words('english')).union(CUSTOM_STOP_WORDS)\n    lemmatizer = WordNetLemmatizer()\n\n    def clean_text(text):\n        if not isinstance(text, str) or len(text.strip()) < 10:\n            return \"\"\n\n        # âœ¨ STEP 1: Detect language and translate\n        try:\n            lang = detect(text)\n            if lang != 'en':\n                # Use the new, more reliable translator\n                text = GoogleTranslator(source='auto', target='en').translate(text)\n        except Exception as e:\n            # If detection or translation fails, just proceed with the original text\n            pass\n\n        # âœ¨ STEP 2: Correct spelling (using TextBlob for this part is generally fine)\n        text = str(TextBlob(text).correct())\n\n        # --- Original cleaning steps ---\n        text = text.lower()\n        text = re.sub(r'[\\d\\n]', '', text)\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        tokens = word_tokenize(text.strip())\n        cleaned_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]\n        return \" \".join(cleaned_tokens)\n\n    df.dropna(subset=[TEXT_COLUMN_NAME], inplace=True)\n    df['cleaned_text'] = df[TEXT_COLUMN_NAME].progress_apply(clean_text)\n\n    # --- 3. Sentiment Analysis ---\n    sia = SentimentIntensityAnalyzer()\n    df['sentiment_compound'] = df[TEXT_COLUMN_NAME].apply(lambda x: sia.polarity_scores(x)['compound'])\n    def categorize_sentiment(compound):\n        if compound >= 0.05: return 'Positive'\n        if compound <= -0.05: return 'Negative'\n        return 'Neutral'\n    df['sentiment_label'] = df['sentiment_compound'].apply(categorize_sentiment)\n\n    print(\"\\n--- Data Preview with Cleaned Text and Sentiment ---\")\n    display(df[[TEXT_COLUMN_NAME, 'cleaned_text', 'sentiment_label']].head())\n    \n# --- 4. Plot Sentiment Distribution and Save ---\n    \n    sentiment_counts = df['sentiment_label'].value_counts()\n    \n    plt.style.use('seaborn-v0_8-deep') \n    plt.figure(figsize=(6, 4))\n    \n    # Plot the data using the new variable\n    sentiment_counts.plot(kind='bar', color=['green', 'red', 'grey'], width=0.9)\n    \n    # ADDED: Loop through the counts to add text labels (call outs)\n    for i, count in enumerate(sentiment_counts):\n        # The plt.text() function places text on the plot\n        # x=i: the horizontal position of the bar (0, 1, 2...)\n        # y=count: the vertical position (the height of the bar)\n        # s=str(count): the text to display (the count as a string)\n        # ha='center': horizontally center the text on the bar\n        plt.text(i, count + (sentiment_counts.max() * 0.01), str(count), ha='center')\n\n    plt.title('Sentiment Distribution')\n    plt.ylabel('Number of Responses')\n    plt.xticks(rotation=0)\n    \n    # Adjust y-axis to make space for the labels\n    plt.ylim(0, sentiment_counts.max() * 1.1) \n\n    fig = plt.gcf()\n    fig.patch.set_edgecolor('grey')\n    fig.patch.set_linewidth(1)\n    \n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, 'sentiment_distribution.png'), bbox_inches='tight')\n    plt.show()\n\nexcept FileNotFoundError:\n    print(\"=\"*80)\n    print(f\"âŒ FATAL ERROR: File not found at the specified path.\")\n    print(f\"    Your specified path: '{FILE_PATH}'\")\n    print(\"    Please check the 'FILE_PATH' variable in your configuration cell and try again.\")\n    print(\"=\"*80)\n    raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Topic Evaluation (Silhouette & Coherence)","metadata":{}},{"cell_type":"code","source":"# --- 1. Prepare Data for Evaluation ---\neval_vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')\ndtm_eval = eval_vectorizer.fit_transform(df['cleaned_text'].dropna())\ntexts_for_gensim = [word_tokenize(text) for text in df['cleaned_text'].dropna()]\ndictionary = Dictionary(texts_for_gensim)\ncorpus = [dictionary.doc2bow(text) for text in texts_for_gensim]\n\n# --- 2. Define a Range of Topics to Test ---\nmin_topics = 2\nmax_topics = 11\ntopic_range = range(min_topics, max_topics)\n\n# --- 3. Calculate Scores for Each Number of Topics ---\nsilhouette_scores = []\ncoherence_scores = []\nprint(\"Evaluating optimal number of topics (k). This may take several minutes...\")\nfor k in topic_range:\n    # Silhouette Score\n    lda_sklearn = LatentDirichletAllocation(n_components=k, random_state=42)\n    lda_sklearn.fit(dtm_eval)\n    if len(np.unique(lda_sklearn.transform(dtm_eval).argmax(axis=1))) > 1:\n        score = silhouette_score(dtm_eval, lda_sklearn.transform(dtm_eval).argmax(axis=1))\n        silhouette_scores.append(score)\n    else:\n        silhouette_scores.append(-1)\n    \n    # Coherence Score\n    lda_gensim = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42)\n    coherence_model = CoherenceModel(model=lda_gensim, texts=texts_for_gensim, dictionary=dictionary, coherence='c_v')\n    coherence = coherence_model.get_coherence()\n    coherence_scores.append(coherence)\n    print(f\"  - Processed k={k} topics...\")\n\n# --- 4. Find the Optimal Number of Topics ---\n# Handle cases with no variance in scores to prevent errors\ns_range = np.max(silhouette_scores) - np.min(silhouette_scores)\nc_range = np.max(coherence_scores) - np.min(coherence_scores)\nnorm_silhouette = (silhouette_scores - np.min(silhouette_scores)) / s_range if s_range > 0 else np.zeros(len(silhouette_scores))\nnorm_coherence = (coherence_scores - np.min(coherence_scores)) / c_range if c_range > 0 else np.zeros(len(coherence_scores))\ncombined_score = norm_silhouette + norm_coherence\nbest_k_index = np.argmax(combined_score)\nrecommended_k = topic_range[best_k_index]\n\n# --- 5. Plot and Save the Results ---\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n# Plot Silhouette\nax1.plot(topic_range, silhouette_scores, marker='o', color='b')\nax1.set_title('Silhouette Score vs. Number of Topics')\nax1.axvline(x=recommended_k, color='grey', linestyle='--', label=f'Recommended k={recommended_k}')\nax1.legend()\n# Plot Coherence\nax2.plot(topic_range, coherence_scores, marker='o', color='r')\nax2.set_title('Topic Coherence (C_v) vs. Number of Topics')\nax2.axvline(x=recommended_k, color='grey', linestyle='--', label=f'Recommended k={recommended_k}')\nax2.legend()\nplt.tight_layout()\n\nfig = plt.gcf()\nfig.patch.set_edgecolor('grey')\nfig.patch.set_linewidth(1)\n\nplt.savefig(os.path.join(OUTPUT_FOLDER_PATH, 'topic_evaluation_charts.png'), bbox_inches='tight')\nplt.show()\n\n# --- 6. Store Results and Update Variable ---\nevaluation_results = {\n    'Num_Topics (k)': list(topic_range),\n    'Silhouette_Score': silhouette_scores,\n    'Coherence_Score': coherence_scores\n}\ndf_topic_evaluation = pd.DataFrame(evaluation_results)\n\n# --- 7. Print Summary and Update TOPIC_MODEL_TOPICS Variable ---\nprint(\"\\n--- Evaluation Results Table ---\")\ndisplay(df_topic_evaluation.round(3))\n\nprint(\"\\n--- Automated Recommendation ---\")\nprint(f\"Best Silhouette Score at k = {topic_range[np.argmax(silhouette_scores)]}\")\nprint(f\"Best Coherence Score at k = {topic_range[np.argmax(coherence_scores)]}\")\nprint(\"-\" * 30)\nprint(f\"ðŸ† Recommended number of topics (best combined score): {recommended_k}\")\nprint(\"-\" * 30)\n\nTOPIC_MODEL_TOPICS = recommended_k\nprint(f\"âœ… The 'TOPIC_MODEL_TOPICS' variable has been automatically set to {TOPIC_MODEL_TOPICS}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Exploratory Analysis (Frequency, N-grams, Word Clouds)\n**This cell prepares frequency tables and word cloud images for the final report.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Prepare Overall Text and Tokens ---\nall_cleaned_text = \" \".join(df['cleaned_text'])\nall_tokens = word_tokenize(all_cleaned_text)\n\n# --- 2. Create and Store Frequency Tables ---\nfdist = FreqDist(all_tokens)\ndf_top_words = pd.DataFrame(fdist.most_common(20), columns=['Word', 'Frequency'])\n\nbigram_fdist = FreqDist(list(bigrams(all_tokens)))\n# CORRECTED: Provide both the bigram and its frequency to the DataFrame\ndf_top_bigrams = pd.DataFrame([(' '.join(gram), freq) for gram, freq in bigram_fdist.most_common(10)], columns=['Bigram', 'Frequency'])\n\ntrigram_fdist = FreqDist(list(trigrams(all_tokens)))\n# CORRECTED: Provide both the trigram and its frequency to the DataFrame\ndf_top_trigrams = pd.DataFrame([(' '.join(gram), freq) for gram, freq in trigram_fdist.most_common(10)], columns=['Trigram', 'Frequency'])\n\nprint(\"--- Top 20 Most Common Words ---\")\ndisplay(df_top_words)\n\nprint(\"--- Top 10 Most Common Bigrams ---\")\ndisplay(df_top_bigrams)\n\nprint(\"--- Top 10 Most Common Trigrams ---\")\ndisplay(df_top_trigrams)\n\n# --- 3. Generate and Save Word Clouds ---\ndef generate_and_save_wordcloud(text, title, filename):\n    if not text.strip():\n        print(f\"Skipping '{title}' word cloud: No text available.\")\n        return\n    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title)\n\n    fig = plt.gcf()\n    fig.patch.set_edgecolor('grey')\n    fig.patch.set_linewidth(1)\n    \n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    plt.show()\n\ngenerate_and_save_wordcloud(all_cleaned_text, 'Word Cloud (All Feedback)', 'wordcloud_all.png')\ngenerate_and_save_wordcloud(\" \".join(df[df.sentiment_label == 'Positive']['cleaned_text']), 'Word Cloud (Positive)', 'wordcloud_positive.png')\ngenerate_and_save_wordcloud(\" \".join(df[df.sentiment_label == 'Negative']['cleaned_text']), 'Word Cloud (Negative)', 'wordcloud_negative.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Thematic Analysis (Topic Modeling and Zero-Shot Categorization)\n**This cell performs the main \"what are they talking about?\" analyses.**","metadata":{}},{"cell_type":"code","source":"# # --- 1. Topic Modeling (LDA) ---\n# print(\"\\n--- Discovering Latent Topics (LDA) ---\")\n# vectorizer = CountVectorizer(max_df=0.9, min_df=3, stop_words='english')\n# dtm = vectorizer.fit_transform(df['cleaned_text'].dropna())\n# if dtm.shape[0] > 1 and dtm.shape[1] > 1:\n#     lda = LatentDirichletAllocation(n_components=TOPIC_MODEL_TOPICS, random_state=42)\n#     lda.fit(dtm)\n#     topic_results = []\n#     feature_names = vectorizer.get_feature_names_out()\n#     for topic_idx, topic in enumerate(lda.components_):\n#         top_words_str = \", \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n#         topic_results.append([f\"Topic #{topic_idx + 1}\", top_words_str])\n#     df_topics = pd.DataFrame(topic_results, columns=['Discovered Topic', 'Top Words'])\n#     display(df_topics)\n# else:\n#     print(\"Not enough data to perform topic modeling.\")\n#     df_topics = pd.DataFrame() # Create empty df if it fails\n\n# # --- 2. Zero-Shot Root Cause Categorization ---\n# print(\"\\n--- Loading Zero-Shot Classification model ---\")\n# classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\",device=0)\n\n# def get_multi_label_predictions(text, labels, threshold):\n#     if not text or not isinstance(text, str): return []\n#     results = classifier(text, candidate_labels=labels, multi_label=True)\n#     return [label for i, label in enumerate(results['labels']) if results['scores'][i] >= threshold]\n\n# def extract_key_phrases(text):\n#     return \"|\".join([str(p) for p in TextBlob(text).noun_phrases[:3]]) if text else \"\"\n\n# def categorize_row(row):\n#     text = row[TEXT_COLUMN_NAME]\n#     matched_cats = get_multi_label_predictions(text, list(CATEGORIES.keys()), CLASSIFICATION_THRESHOLD)\n#     matched_subcats = []\n#     if matched_cats:\n#         for cat in matched_cats:\n#             sub_preds = get_multi_label_predictions(text, CATEGORIES.get(cat, []), CLASSIFICATION_THRESHOLD)\n#             matched_subcats.extend(sub_preds)\n#         if not matched_subcats and (key_phrases := extract_key_phrases(text)):\n#             matched_subcats.append(f\"SUGGESTION: {key_phrases}\")\n#     return \"|\".join(matched_cats) if matched_cats else 'Uncategorized', \"|\".join(matched_subcats) if matched_subcats else \"\"\n\n# print(f\"\\n--- Starting Zero-Shot categorization with a threshold of {CLASSIFICATION_THRESHOLD:.2f} ---\")\n# df[['Category', 'Sub-Category']] = df.progress_apply(categorize_row, axis=1, result_type='expand')\n\n# print(\"\\n--- Categorization Complete ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Revised 5.0.1","metadata":{}},{"cell_type":"markdown","source":"# Cell 6: Deep-Dive Categorization Analysis\n**This cell is dedicated to analyzing the results of our categorization, creating the tables and charts needed for the executive summary.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Create Exploded DataFrames (for re-use) ---\ndf_exploded_cat = df.assign(Category=df['Category'].str.split('|')).explode('Category')\n\ndf_exploded_subcat = df.copy()\ndf_exploded_subcat['Sub-Category'] = df_exploded_subcat['Sub-Category'].str.split('|')\ndf_exploded_subcat = df_exploded_subcat.explode('Sub-Category')\n\ndf_exploded_subcat.dropna(subset=['Sub-Category'], inplace=True)\ndf_exploded_subcat = df_exploded_subcat[df_exploded_subcat['Sub-Category'].str.strip() != '']\n\n# --- 2. Categorization Summary ---\ntotal_rows = len(df)\nuncategorized_count = len(df[df['Category'] == 'Uncategorized'])\ncategorized_count = total_rows - uncategorized_count\ncategorization_rate = (categorized_count / total_rows) * 100 if total_rows > 0 else 0\nprint(\"\\n--- Categorization Summary ---\")\nsummary_metrics = {\n    'Metric': ['Total Verbatims', 'Categorized', 'Uncategorized', 'Categorization Rate'],\n    'Value': [total_rows, categorized_count, uncategorized_count, f\"{categorization_rate:.2f}%\"]\n}\ndf_summary_metrics = pd.DataFrame(summary_metrics)\ndisplay(df_summary_metrics)\n\n# --- 3. Sentiment Breakdown Tables ---\nprint(\"\\n--- Sentiment Breakdown by Category ---\")\ndf_sentiment_by_cat = pd.crosstab(df_exploded_cat['Category'], df_exploded_cat['sentiment_label'])\ndf_sentiment_by_cat['Total'] = df_sentiment_by_cat.sum(axis=1)\ndf_sentiment_by_cat.sort_values(by='Total', ascending=False, inplace=True)\ndisplay(df_sentiment_by_cat.drop(columns='Total'))\n\nprint(\"\\n--- Sentiment Breakdown by Sub-Category ---\")\ndf_sentiment_by_subcat = pd.crosstab(df_exploded_subcat['Sub-Category'], df_exploded_subcat['sentiment_label'])\ndf_sentiment_by_subcat['Total'] = df_sentiment_by_subcat.sum(axis=1)\ndf_sentiment_by_subcat.sort_values(by='Total', ascending=False, inplace=True)\ndisplay(df_sentiment_by_subcat.drop(columns='Total'))\n\n# --- 4. Top Keywords for \"Uncategorized\" Verbatims ---\nprint(\"\\n--- Top Keywords in Uncategorized Verbatims ---\")\nuncategorized_text = \" \".join(df[df['Category'] == 'Uncategorized']['cleaned_text'])\nif uncategorized_text.strip():\n    uncategorized_fdist = FreqDist(word_tokenize(uncategorized_text))\n    df_uncategorized_keywords = pd.DataFrame(uncategorized_fdist.most_common(20), columns=['Keyword', 'Frequency'])\n    display(df_uncategorized_keywords.head(10))\nelse:\n    print(\"No keywords to display for uncategorized verbatims.\")\n    df_uncategorized_keywords = pd.DataFrame(columns=['Keyword', 'Frequency'])\n\n# --- 5. Category and Sub-Category Frequency Analysis & Visualization (UPDATED) ---\n\n# âœ¨ UPDATED function to accept total_verbatims and plot percentages\ndef plot_and_save_top_n(series, title, filename, total_verbatims, n=15):\n    \"\"\"Helper function to plot and save frequency charts as percentages.\"\"\"\n    if series.empty or total_verbatims == 0:\n        print(f\"Skipping plot '{title}': No data.\")\n        return\n        \n    # Convert counts to percentage\n    series_pct = series.head(n).sort_values(ascending=True) / total_verbatims\n    \n    plt.figure(figsize=(12, 8))\n    ax = series_pct.plot(kind='barh', color='darkslateblue', width=0.9)\n    \n    # Format the x-axis as percentages\n    ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n    \n    # --- ADDED: Loop to add data call outs ---\n    for i, value in enumerate(series_pct):\n        # ax.text() places text on the axes\n        # x-coordinate: value (end of the bar) + a small offset\n        # y-coordinate: i (the position of the bar, 0, 1, 2...)\n        # text: formatted as a percentage string (e.g., '15.2%')\n        # va='center': vertically aligns the text with the middle of the bar\n        ax.text(value + (series_pct.max() * 0.01), i, f'{value:.1%}', va='center')\n\n    plt.title(title)\n    plt.xlabel('Percentage of Total Verbatims (%)') # Updated label\n    \n    # Adjust x-axis limit to make space for the labels\n    ax.set_xlim(right=series_pct.max() * 1.15)\n    \n    plt.subplots_adjust(left=0.4)\n    \n    fig = plt.gcf()\n    fig.patch.set_edgecolor('grey')\n    fig.patch.set_linewidth(1)\n\n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    plt.show()\n\n# --- Calculate Overall Frequencies ---\nprint(\"\\n--- Calculating and Plotting Overall Frequencies ---\")\ntotal_verbatims = len(df) # Get total for calculations\ndf_cat_counts = df_exploded_cat['Category'].value_counts()\ndf_subcat_counts = df_exploded_subcat['Sub-Category'].value_counts()\n\n# âœ¨ UPDATED function calls to pass total_verbatims\nplot_and_save_top_n(df_cat_counts.drop('Uncategorized', errors='ignore'), 'Overall Top Categories', 'freq_cat_overall.png', total_verbatims)\nplot_and_save_top_n(df_subcat_counts, 'Overall Top Sub-Categories', 'freq_subcat_overall.png', total_verbatims)\n\n# --- Calculate Frequencies Split by Sentiment ---\nprint(\"\\n--- Calculating and Plotting Frequencies by Sentiment ---\")\nfor sentiment in ['Positive', 'Negative', 'Neutral']:\n    df_sentiment_exploded_cat = df_exploded_cat[df_exploded_cat['sentiment_label'] == sentiment]\n    if not df_sentiment_exploded_cat.empty:\n        cat_counts = df_sentiment_exploded_cat['Category'].value_counts()\n        # âœ¨ UPDATED function calls\n        plot_and_save_top_n(cat_counts, f'Top Categories ({sentiment} Sentiment)', f'freq_cat_{sentiment.lower()}.png', total_verbatims)\n\n    df_sentiment_exploded_subcat = df_exploded_subcat[df_exploded_subcat['sentiment_label'] == sentiment]\n    if not df_sentiment_exploded_subcat.empty:\n        subcat_counts = df_sentiment_exploded_subcat['Sub-Category'].value_counts()\n        # âœ¨ UPDATED function calls\n        plot_and_save_top_n(subcat_counts, f'Top Sub-Categories ({sentiment} Sentiment)', f'freq_subcat_{sentiment.lower()}.png', total_verbatims)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7: Breakdown of Sub-categories by Sentiment and Category","metadata":{}},{"cell_type":"code","source":"print(\"--- ðŸ“Š Generating Validated Breakdown, Plots, & Examples ---\")\n\n# --- [Keep the sanitize_filename helper function as is] ---\ndef sanitize_filename(name):\n    name = re.sub(r'[^\\w\\s-]', '', name).strip().lower()\n    name = re.sub(r'[-\\s]+', '_', name)\n    return name\n\n# âœ¨ NEW: Get the total number of verbatims for percentage calculations\ntotal_verbatims = len(df)\nprint(f\"Total verbatims being processed for percentages: {total_verbatims}\")\n\n# --- [Keep the data preparation steps as they were] ---\nvalid_pairs = set()\nfor category, subcategories in CATEGORIES.items():\n    for subcategory in subcategories:\n        valid_pairs.add((category, subcategory))\n\ndf_exploded_all_links = df.assign(Category=df['Category'].str.split('|')).explode('Category')\ndf_exploded_all_links = df_exploded_all_links.assign(SubCategory=df_exploded_all_links['Sub-Category'].str.split('|')).explode('SubCategory')\ndf_exploded_all_links.dropna(subset=['Category', 'SubCategory'], inplace=True)\ndf_exploded_all_links = df_exploded_all_links[\n    (df_exploded_all_links['Category'].str.strip() != '') &\n    (df_exploded_all_links['SubCategory'].str.strip() != '')\n]\ndf_validated_links = df_exploded_all_links[\n    df_exploded_all_links.apply(lambda row: (row['Category'], row['SubCategory']) in valid_pairs, axis=1)\n]\n\n# --- Perform crosstabulation and display results ---\nif not df_validated_links.empty:\n    granular_sentiment_df = pd.crosstab(\n        [df_validated_links['Category'], df_validated_links['SubCategory']],\n        df_validated_links['sentiment_label']\n    )\n    for sentiment in ['Negative', 'Neutral', 'Positive']:\n        if sentiment not in granular_sentiment_df.columns:\n            granular_sentiment_df[sentiment] = 0\n    granular_sentiment_df['Total'] = granular_sentiment_df.sum(axis=1)\n    \n    unique_categories = granular_sentiment_df.index.get_level_values('Category').unique()\n\n    for category in unique_categories:\n        print(\"\\n\" + \"#\"*80)\n        print(f\"## CATEGORY: {category}\")\n        print(\"#\"*80)\n        \n        category_view = granular_sentiment_df.loc[category].sort_values(by='Total', ascending=False)\n        category_view = category_view[['Negative', 'Neutral', 'Positive', 'Total']]\n        display(category_view)\n        \n        # --- âœ¨ NEW: VERBATIM EXAMPLES SECTION ---\n        print(\"\\n\" + \"-\"*40)\n        print(\"Verbatim Examples:\")\n        print(\"-\"*40)\n        # Loop through the sub-categories in the table we just displayed\n        for sub_cat in category_view.index:\n            # Find the original verbatims that match this exact category and sub-category\n            examples = df_validated_links[\n                (df_validated_links['Category'] == category) &\n                (df_validated_links['SubCategory'] == sub_cat)\n            ][TEXT_COLUMN_NAME]\n            \n            # Take a random sample of up to 3 examples\n            sample_size = min(len(examples), 3)\n            if sample_size > 0:\n                print(f\"\\nâ–¶ Sub-Category: '{sub_cat}'\")\n                for i, verbatim in enumerate(examples.sample(sample_size, random_state=42)):\n                    print(f\"  {i+1}. \\\"{verbatim}\\\"\")\n        \n        # --- âœ¨ UPDATED: PLOTTING SECTION (NOW WITH PERCENTAGES) ---\n        for sentiment in ['Negative', 'Positive', 'Neutral']:\n            plot_data = category_view[sentiment][category_view[sentiment] > 0].sort_values(ascending=True)\n            \n            if not plot_data.empty:\n                # Convert counts to percentage of total\n                plot_data_pct = plot_data / total_verbatims\n                \n                plt.figure(figsize=(10, len(plot_data) * 0.5 + 2))\n                ax = plot_data_pct.plot(kind='barh', color={'Negative':'salmon', 'Positive':'mediumseagreen', 'Neutral':'silver'}[sentiment], width=0.9)\n                \n                # Format the x-axis as percentages\n                ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n\n                # --- ADDED: Loop to add data call outs ---\n                for i, value in enumerate(plot_data_pct):\n                    # ax.text() places text on the axes\n                    # x-coordinate: value (end of the bar) + a small offset\n                    # y-coordinate: i (the position of the bar, 0, 1, 2...)\n                    # text: formatted as a percentage string (e.g., '5.2%')\n                    ax.text(value + (plot_data_pct.max() * 0.01), i, f'{value:.1%}', va='center')\n\n                plt.title(f\"{sentiment} Feedback for:\\n'{category}'\", fontsize=14)\n                plt.xlabel(\"Percentage of Total Verbatims (%)\", fontsize=12)\n                plt.ylabel(\"\")\n\n                # ADDED: Adjust x-axis limit to make space for the labels\n                ax.set_xlim(right=plot_data_pct.max() * 1.18)\n\n                plt.tight_layout()\n                \n                s_cat = sanitize_filename(category)\n                s_sent = sanitize_filename(sentiment)\n                filename = f\"granular_{s_cat}_{s_sent}.png\"\n                                \n                fig = plt.gcf()\n                fig.patch.set_edgecolor('grey')\n                fig.patch.set_linewidth(1)\n\n                plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename))\n                plt.show()\nelse:\n    print(\"No valid Category/Sub-Category links found to generate a granular view.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8: Similarity Analysis (Jaccard & Semantic)\n**This cell performs the deeper analysis on vocabulary overlap and semantic relationships.**","metadata":{}},{"cell_type":"code","source":"# --- 1. Jaccard Similarity Between Sentiments ---\ndef get_top_ngrams(tokens, n, N_top):\n    \"\"\"Extracts the top N n-grams from a list of tokens.\"\"\"\n    ngrams_list = tokens if n == 1 else list(nltk.ngrams(tokens, n))\n    return [item for item, freq in FreqDist(ngrams_list).most_common(N_top)]\n\ndef jaccard_similarity(list1, list2):\n    \"\"\"Calculates Jaccard similarity between two lists.\"\"\"\n    set1, set2 = set(list1), set(list2)\n    intersection_len = len(set1.intersection(set2))\n    union_len = len(set1.union(set2))\n    return intersection_len / union_len if union_len > 0 else 0.0\n\n# Calculate Jaccard similarity for top words and bigrams between sentiments\nsentiments = ['Positive', 'Negative', 'Neutral']\ntokens_by_sentiment = {s: word_tokenize(\" \".join(df[df.sentiment_label == s]['cleaned_text'])) for s in sentiments}\ncomparisons = [(\"Positive\", \"Negative\"), (\"Positive\", \"Neutral\"), (\"Negative\", \"Neutral\")]\n\nresults = {\n    \"Comparison\": [f\"{s1} vs. {s2}\" for s1, s2 in comparisons],\n    \"Words (Top 20)\": [jaccard_similarity(get_top_ngrams(tokens_by_sentiment[s1], 1, 20), get_top_ngrams(tokens_by_sentiment[s2], 1, 20)) for s1, s2 in comparisons],\n    \"Bigrams (Top 10)\": [jaccard_similarity(get_top_ngrams(tokens_by_sentiment[s1], 2, 10), get_top_ngrams(tokens_by_sentiment[s2], 2, 10)) for s1, s2 in comparisons],\n    \"Trigrams (Top 10)\": [jaccard_similarity(get_top_ngrams(tokens_by_sentiment[s1], 3, 10), get_top_ngrams(tokens_by_sentiment[s2], 3, 10)) for s1, s2 in comparisons],\n}\nsimilarity_df = pd.DataFrame(results)\n\n# --- 2. Inter-Item Semantic & Jaccard Similarity ---\nnlp = spacy.load(\"en_core_web_md\")\ntop_words_overall = get_top_ngrams(all_tokens, 1, 20) # Top 20 Words\ntop_bigrams_overall = get_top_ngrams(all_tokens, 2, 10) # Top 10 Bigrams\ntop_trigrams_overall = get_top_ngrams(all_tokens, 3, 10) # Top 10 Trigrams\n\n# Word vs Word (Semantic Similarity)\nmatrix_words = np.array([[nlp(w1).similarity(nlp(w2)) for w2 in top_words_overall] for w1 in top_words_overall])\nsimilarity_df_words = pd.DataFrame(matrix_words, index=top_words_overall, columns=top_words_overall)\n\n# Words vs Bigrams (Jaccard Similarity)\nmatrix_words_vs_bigrams = np.array([[jaccard_similarity([word], list(bigram)) for bigram in top_bigrams_overall] for word in top_words_overall])\nsimilarity_df_words_vs_bigrams = pd.DataFrame(matrix_words_vs_bigrams, index=top_words_overall, columns=[' '.join(g) for g in top_bigrams_overall])\n\n# Words vs Trigrams (Jaccard Similarity)\nmatrix_words_vs_trigrams = np.array([[jaccard_similarity([word], list(trigram)) for trigram in top_trigrams_overall] for word in top_words_overall])\nsimilarity_df_words_vs_trigrams = pd.DataFrame(matrix_words_vs_trigrams, index=top_words_overall, columns=[' '.join(g) for g in top_trigrams_overall])\n\n# Bigram vs Bigram (Jaccard Similarity)\nmatrix_bg = np.array([[jaccard_similarity(list(g1), list(g2)) for g2 in top_bigrams_overall] for g1 in top_bigrams_overall])\nsimilarity_df_bigrams = pd.DataFrame(matrix_bg, index=[' '.join(g) for g in top_bigrams_overall], columns=[' '.join(g) for g in top_bigrams_overall])\n\n# Trigram vs Trigram (Jaccard Similarity)\nmatrix_tg = np.array([[jaccard_similarity(list(g1), list(g2)) for g2 in top_trigrams_overall] for g1 in top_trigrams_overall])\nsimilarity_df_trigrams = pd.DataFrame(matrix_tg, index=[' '.join(g) for g in top_trigrams_overall], columns=[' '.join(g) for g in top_trigrams_overall])\n\n\n# --- 3. Generate and Save Heatmap Images ---\ndef create_and_save_heatmap(df_plot, title, filename, annot=False, cmap='viridis', figsize=(12, 10)):\n    \"\"\"Creates, displays, and saves a heatmap from a DataFrame.\"\"\"\n    plt.figure(figsize=figsize)\n    sns.heatmap(df_plot, annot=annot, cmap=cmap, fmt=\".2f\")\n    plt.title(title, fontsize=16)\n        \n    fig = plt.gcf()\n    fig.patch.set_edgecolor('grey')\n    fig.patch.set_linewidth(1)\n\n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    plt.show()\n\n# Generate and save the heatmaps\ncreate_and_save_heatmap(similarity_df_words, 'Semantic Similarity of Top 20 Words', 'heatmap_words.png')\ncreate_and_save_heatmap(similarity_df_bigrams, 'Jaccard Similarity of Top 10 Bigrams', 'heatmap_bigrams.png', annot=True, cmap='coolwarm')\n\n# ADDED: Heatmap for Trigrams vs Trigrams\ncreate_and_save_heatmap(similarity_df_trigrams, 'Jaccard Similarity of Top 10 Trigrams', 'heatmap_trigrams.png', annot=True, cmap='coolwarm')\n\n# ADDED: Heatmap for Words vs Bigrams\ncreate_and_save_heatmap(similarity_df_words_vs_bigrams, 'Jaccard Similarity: Top Words vs. Top Bigrams', 'heatmap_words_vs_bigrams.png', annot=True, cmap='magma', figsize=(10, 12))\n\n# ADDED: Heatmap for Words vs Trigrams\ncreate_and_save_heatmap(similarity_df_words_vs_trigrams, 'Jaccard Similarity: Top Words vs. Top Trigrams', 'heatmap_words_vs_trigrams.png', annot=True, cmap='magma', figsize=(10, 12))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8.5: Categories & Sub-categories to Columns","metadata":{}},{"cell_type":"code","source":"# --- One-Hot Encode Categories for Easy Filtering ---\nprint(\"--- âœ¨ Creating One-Hot Encoded Columns for Categories & Sub-Categories ---\")\n\n# Step 1: Get a unique list of all possible categories and sub-categories from your config\nall_categories = list(CATEGORIES.keys())\nall_subcategories = [subcat for sublist in CATEGORIES.values() for subcat in sublist]\n\n# Step 2: Create a new column for each CATEGORY\n# For each category, check if the string exists in the 'Category' column.\n# .str.contains() returns True/False, which we convert to 1/0 with .astype(int).\n# We use re.escape to handle any special characters in your category names safely.\nprint(\"Encoding main categories...\")\nfor cat in tqdm(all_categories, desc=\"Encoding Categories\"):\n    # Ensure the column doesn't already exist to prevent errors on re-runs\n    if cat not in df.columns:\n        df[cat] = df['Category'].str.contains(re.escape(cat), na=False).astype(int)\n\n# Step 3: Create a new column for each SUB-CATEGORY\nprint(\"Encoding sub-categories...\")\nfor subcat in tqdm(all_subcategories, desc=\"Encoding Sub-Categories\"):\n     # Ensure the column doesn't already exist\n    if subcat not in df.columns:\n        df[subcat] = df['Sub-Category'].str.contains(re.escape(subcat), na=False).astype(int)\n\n# Step 4: Display a preview of the new columns\nprint(\"\\nâœ… One-hot encoding complete.\")\nprint(\"--- Preview of DataFrame with New Encoded Columns ---\")\n# Create a list of columns to show: original text, categories, and the first few new columns\npreview_cols = [TEXT_COLUMN_NAME, 'Category', 'Sub-Category'] + all_categories[:2] + all_subcategories[:2]\ndisplay(df[preview_cols].head())\n\n# --- Run Significance Testing (T-tests) ---\nSCORE_COLUMN_NAME = 'CSAT Score'\n\nprint(f\"--- ðŸƒ Running Significance Tests against '{SCORE_COLUMN_NAME}' column ---\")\n\n# --- Safety Check ---\nif SCORE_COLUMN_NAME not in df.columns:\n    print(\"=\"*80)\n    print(f\"âŒ FATAL ERROR: The score column '{SCORE_COLUMN_NAME}' was not found in the DataFrame.\")\n    print(\"   Please update the 'SCORE_COLUMN_NAME' variable in this cell and run it again.\")\n    print(\"=\"*80)\nelse:\n    # âœ¨ Step 1: Create a mapping of each sub-category to its parent category\n    all_categories = list(CATEGORIES.keys())\n    subcat_to_cat_map = {\n        subcat: cat \n        for cat, sublist in CATEGORIES.items() \n        for subcat in sublist\n    }\n\n    # âœ¨ Step 2: Run tests for Categories ONLY\n    category_results = []\n    for cat_col in tqdm(all_categories, desc=\"Testing Level 1 Categories\"):\n        if cat_col in df.columns:\n            group_present = df[df[cat_col] == 1][SCORE_COLUMN_NAME].dropna()\n            group_absent = df[df[cat_col] == 0][SCORE_COLUMN_NAME].dropna()\n            if len(group_present) > 1 and len(group_absent) > 1:\n                t_stat, p_value = stats.ttest_ind(group_present, group_absent, equal_var=False)\n                category_results.append({\n                    'Category': cat_col,\n                    'Mean Score When Present': group_present.mean(),\n                    'Mean Score When Absent': group_absent.mean(),\n                    'p-value': p_value,\n                    'Comment Count': len(group_present)\n                })\n\n    # âœ¨ Step 3: Run tests for Sub-Categories ONLY\n    sub_category_results = []\n    for subcat_col, parent_cat in tqdm(subcat_to_cat_map.items(), desc=\"Testing Level 2 Sub-Categories\"):\n        if subcat_col in df.columns:\n            group_present = df[df[subcat_col] == 1][SCORE_COLUMN_NAME].dropna()\n            group_absent = df[df[subcat_col] == 0][SCORE_COLUMN_NAME].dropna()\n            if len(group_present) > 1 and len(group_absent) > 1:\n                t_stat, p_value = stats.ttest_ind(group_present, group_absent, equal_var=False)\n                sub_category_results.append({\n                    'Parent Category': parent_cat,\n                    'Sub-Category': subcat_col,\n                    'Mean Score When Present': group_present.mean(),\n                    'Mean Score When Absent': group_absent.mean(),\n                    'p-value': p_value,\n                    'Comment Count': len(group_present)\n                })\n\n    # --- âœ¨ Step 4: Assemble the three final tables ---\n\n    # Table 1: Category-Only Results\n    df_significance_categories = pd.DataFrame(category_results)\n    if not df_significance_categories.empty:\n        df_significance_categories['Significant (p < 0.05)'] = df_significance_categories['p-value'] < 0.05\n        df_significance_categories = df_significance_categories.sort_values(by='p-value', ascending=True)\n\n    # Table 2: Sub-Category-Only Results\n    df_significance_subcategories = pd.DataFrame(sub_category_results)\n    if not df_significance_subcategories.empty:\n        df_significance_subcategories['Significant (p < 0.05)'] = df_significance_subcategories['p-value'] < 0.05\n        df_significance_subcategories = df_significance_subcategories.sort_values(by='p-value', ascending=True)\n\n    # Table 3: Overall Combined Results\n    df_overall_cat = df_significance_categories.copy()\n    df_overall_cat['Driver'] = 'Level 1: ' + df_overall_cat['Category']\n    \n    df_overall_sub = df_significance_subcategories.copy()\n    df_overall_sub['Driver'] = 'Level 2: ' + df_overall_sub['Sub-Category']\n    \n    df_significance_overall = pd.concat([\n        df_overall_cat.drop(columns=['Category']),\n        df_overall_sub.drop(columns=['Parent Category', 'Sub-Category'])\n    ])\n    if not df_significance_overall.empty:\n        # Reorder columns to put Driver first\n        cols = ['Driver'] + [col for col in df_significance_overall.columns if col != 'Driver']\n        df_significance_overall = df_significance_overall[cols]\n        df_significance_overall = df_significance_overall.sort_values(by='p-value', ascending=True)\n\n    # --- âœ¨ Step 5: Display all three tables ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"      SUMMARY OF SIGNIFICANCE TEST RESULTS\")\n    print(\"=\"*50 + \"\\n\")\n\n    print(\"\\n--- ðŸ“œ Table 1: Overall Significance (Combined) ---\")\n    display(df_significance_overall.round(3))\n    \n    print(\"\\n--- ðŸ“‚ Table 2: Category-Only Significance ---\")\n    display(df_significance_categories.round(3))\n\n    print(\"\\n--- ðŸ“„ Table 3: Sub-Category Significance (with Parent) ---\")\n    display(df_significance_subcategories.round(3))\n\n    # This ensures the next cell (for plotting) uses the main combined table\n    df_significance = df_significance_overall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8.6: Impact-Significance Plot","metadata":{}},{"cell_type":"code","source":"# âœ¨ Import the new library\ntry:\n    from adjustText import adjust_text\nexcept ImportError:\n    !pip install adjusttext\n    from adjustText import adjust_text\n\n# --- 10. Create Separate Impact-Significance Matrix Plots ---\n\n# âœ¨ Step 1: Define a reusable function to create the plot\ndef create_impact_plot(df_data, name_column, title, filename):\n    \"\"\"\n    Generates and saves an impact-significance scatter plot with auto-adjusting labels.\n    \"\"\"\n    print(f\"--- ðŸ“Š Generating Plot: {title} ---\")\n    \n    if df_data is None or df_data.empty:\n        print(f\"âš ï¸ Skipping plot '{title}': Input DataFrame is empty or does not exist.\")\n        return\n\n    df_plot_data = df_data.copy()\n    df_plot_data['Score_Difference'] = df_plot_data['Mean Score When Present'] - df_plot_data['Mean Score When Absent']\n    \n    bins = [0, 0.05, 0.1, 1.01]\n    labels = ['p < 0.05 (Significant)', '0.05 <= p < 0.1', 'p >= 0.1 (Not Significant)']\n    df_plot_data['Significance Level'] = pd.cut(df_plot_data['p-value'], bins=bins, labels=labels, right=False)\n\n    points_to_label = df_plot_data.sort_values(name_column).copy()\n    points_to_label['Label_ID'] = range(1, len(points_to_label) + 1)\n    \n    plt.figure(figsize=(18, 12))\n    \n    # âœ¨ CHANGE #1: Add legend='brief' to simplify the size legend\n    ax = sns.scatterplot(\n        data=df_plot_data,\n        x='Score_Difference',\n        y='p-value',\n        hue='Significance Level',\n        palette={\n            'p < 0.05 (Significant)': 'springgreen',\n            '0.05 <= p < 0.1': 'orange',\n            'p >= 0.1 (Not Significant)': 'grey'\n        },\n        size='Comment Count',\n        sizes=(50, 500),\n        alpha=0.7,\n        legend='brief' \n    )\n\n    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n    plt.axhline(y=0.05, color='green', linestyle='--', linewidth=1)\n    plt.axhline(y=0.10, color='darkorange', linestyle='--', linewidth=1)\n    plt.title(title, fontsize=16)\n    plt.xlabel(\"Impact on Score (Mean Score Difference)\", fontsize=12)\n    plt.ylabel(\"p-value (Statistical Significance)\", fontsize=12)\n    plt.ylim(-0.05, max(1.0, df_plot_data['p-value'].max() * 1.05))\n\n    texts = []\n    for i, row in points_to_label.iterrows():\n        texts.append(\n            plt.text(\n                x=row['Score_Difference'],\n                y=row['p-value'],\n                s=f\" {row['Label_ID']}\",\n                fontdict=dict(color='black', weight='bold', size=8)\n            )\n        )\n        \n    plt.text(0.01, 0.05, ' p=0.05 (Significance Threshold)', color='green', ha='left', va='bottom', fontsize=10, style='italic')\n    plt.text(0.01, 0.10, ' p=0.10 (Significance at 90% Confidence Threshold)', color='darkorange', ha='left', va='bottom', fontsize=10, style='italic')\n    \n    if not points_to_label.empty:\n        legend_text = \"Labels:\\n\" + \"\\n\".join(\n            [f\"{row['Label_ID']}: {row[name_column]}\" for i, row in points_to_label.iterrows()]\n        )\n        \n        # Position the main color and size legends\n        plt.legend(title='Legend', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n        \n        # âœ¨ CHANGE #2: Move the custom text box further down (y=0.7)\n        plt.text(1.02, 0.70, legend_text, transform=ax.transAxes, fontsize=9,\n                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.9))\n\n    adjust_text(texts, arrowprops=dict(arrowstyle='->', color='black', lw=0.5))\n\n    plt.subplots_adjust(right=0.7) \n    fig = plt.gcf()\n    fig.patch.set_edgecolor('grey')\n    fig.patch.set_linewidth(1)\n    \n    plt.savefig(os.path.join(OUTPUT_FOLDER_PATH, filename), bbox_inches='tight')\n    print(f\"âœ… Plot saved as '{filename}'\")\n    plt.show()\n\n# âœ¨ Step 2: Call the function twice, once for categories and once for sub-categories\n# (This part of the code remains the same)\ncreate_impact_plot(\n    df_data=df_significance_categories,\n    name_column='Category',\n    title=f\"Impact vs. Significance Matrix for Categories\",\n    filename='impact_matrix_categories.png'\n)\n\ncreate_impact_plot(\n    df_data=df_significance_subcategories,\n    name_column='Sub-Category',\n    title=f\"Impact vs. Significance Matrix for Sub-Categories\",\n    filename='impact_matrix_subcategories.png'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 9: Final Report Generation\n**This final cell gathers every DataFrame and image and compiles them into a single, multi-sheet Excel report.**","metadata":{}},{"cell_type":"code","source":"# --- Helper Functions ---\ndef sanitize_filename(name):\n    name = re.sub(r'[^\\w\\s-]', '', name).strip().lower()\n    name = re.sub(r'[-\\s]+', '_', name)\n    return name\n\ndef add_image_if_exists(worksheet, image_filename, cell_anchor, folder_path):\n    image_path = os.path.join(folder_path, image_filename)\n    if os.path.exists(image_path):\n        img = Image(image_path)\n        worksheet.add_image(img, cell_anchor)\n    else:\n        print(f\"Warning: Image file not found at {image_path}, skipping.\")\n\ndef apply_borders(ws, start_row, end_row, start_col, end_col):\n    \"\"\"Applies a thin border to all cells in a given range.\"\"\"\n    thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))\n    for row in ws.iter_rows(min_row=start_row, max_row=end_row, min_col=start_col, max_col=end_col):\n        for cell in row:\n            cell.border = thin_border\n\ndef write_df_with_borders(writer, sheet_name, df, start_row=0, **kwargs):\n    \"\"\"Writes a DataFrame to Excel and applies borders to all its cells.\"\"\"\n    df.to_excel(writer, sheet_name=sheet_name, startrow=start_row, **kwargs)\n    ws = writer.sheets[sheet_name]\n    index_cols = df.index.nlevels if kwargs.get('index', True) else 0\n    end_col = index_cols + len(df.columns)\n    apply_borders(ws, start_row + 1, start_row + 1 + len(df), 1, end_col)\n\n# --- Versioned Filename Generation ---\ncurrent_date = datetime.now().strftime('%Y-%m-%d')\nbase_filename = f\"{current_date}_{PROGRAM_NAME}_{KPIS_IN_SCOPE}_{LOBS_IN_SCOPE}_Verbatim_Analysis\"\nminor_version = 0\nwhile True:\n    version_str = f\"v{MAJOR_VERSION:02d}.{minor_version:02d}\"\n    output_filename = f\"{base_filename}_{version_str}.xlsx\"\n    full_path = os.path.join(OUTPUT_FOLDER_PATH, output_filename)\n    if not os.path.exists(full_path):\n        break\n    minor_version += 1\n\n# --- Use ExcelWriter to save all results ---\nwith pd.ExcelWriter(full_path, engine='openpyxl') as writer:\n    print(f\"\\n--- ðŸš€ Writing to Excel file: {output_filename} ---\")\n\n    # --- Sheet 1: Executive Summary ---\n    print(\"Writing Sheet: Executive_Summary\")\n    row_offset = 0\n    write_df_with_borders(writer, 'Executive_Summary', df_summary_metrics, start_row=row_offset, index=False)\n    pos1 = row_offset + len(df_summary_metrics) + 3\n    write_df_with_borders(writer, 'Executive_Summary', df_topic_evaluation.round(3), start_row=pos1, index=False)\n    pos2 = pos1 + len(df_topic_evaluation) + 3\n    write_df_with_borders(writer, 'Executive_Summary', df_cat_counts.to_frame(name='Count'), start_row=pos2)\n    pos3 = pos2 + len(df_cat_counts) + 3\n    write_df_with_borders(writer, 'Executive_Summary', df_subcat_counts.to_frame(name='Count'), start_row=pos3)\n    ws_summary = writer.sheets['Executive_Summary']\n    ws_summary.cell(row=row_offset + 1, column=1, value=\"Categorization Summary\")\n    ws_summary.cell(row=pos1 + 1, column=1, value=\"Topic Model Evaluation (Silhouette & Coherence)\")\n    ws_summary.cell(row=pos2 + 1, column=1, value=\"Overall Category Counts\")\n    ws_summary.cell(row=pos3 + 1, column=1, value=\"Overall Sub-Category Counts\")\n    add_image_if_exists(ws_summary, 'topic_evaluation_charts.png', 'E2', OUTPUT_FOLDER_PATH)\n    add_image_if_exists(ws_summary, 'freq_cat_overall.png', f'E{pos2 + 2}', OUTPUT_FOLDER_PATH)\n    add_image_if_exists(ws_summary, 'freq_subcat_overall.png', f'E{pos3 + 2}', OUTPUT_FOLDER_PATH)\n\n    # --- Sheet 2: Category Deep-Dive ---\n    sheet_name = 'Category_Deep_Dive'\n    print(f\"Writing Sheet: {sheet_name}\")\n    if 'granular_sentiment_df' in locals() and not granular_sentiment_df.empty:\n        current_row = 1\n        unique_categories = granular_sentiment_df.index.get_level_values('Category').unique()\n\n        for category in unique_categories:\n            category_view = granular_sentiment_df.loc[category].sort_values(by='Total', ascending=False)\n            category_view = category_view[['Negative', 'Neutral', 'Positive', 'Total']]\n            \n            examples_list = []\n            for sub_cat in category_view.index:\n                examples = df_validated_links[(df_validated_links['Category'] == category) & (df_validated_links['SubCategory'] == sub_cat)][TEXT_COLUMN_NAME]\n                sample_size = min(len(examples), 3)\n                if sample_size > 0:\n                    formatted_examples = \"\\n\".join([f\"{j+1}. \\\"{v}\\\"\" for j, v in enumerate(examples.sample(sample_size, random_state=42))])\n                    examples_list.append(formatted_examples)\n                else:\n                    examples_list.append(\"No examples found.\")\n            category_view['Verbatim Examples'] = examples_list\n            \n            write_df_with_borders(writer, sheet_name, category_view, start_row=current_row)\n            \n            ws_deep_dive = writer.sheets[sheet_name]\n            ws_deep_dive.cell(row=current_row, column=1, value=f\"Deep-Dive for Category: {category}\")\n            \n            # Format Column A (Sub-Category)\n            ws_deep_dive.column_dimensions['A'].width = 30\n            for r in range(current_row + 2, current_row + 2 + len(category_view)):\n                ws_deep_dive[f'A{r}'].alignment = Alignment(wrap_text=True, vertical='top')\n\n            # Format Column G (Verbatim Examples)\n            ws_deep_dive.column_dimensions['F'].width = 90\n            for r in range(current_row + 2, current_row + 2 + len(category_view)):\n                ws_deep_dive[f'G{r}'].alignment = Alignment(wrap_text=True, vertical='top')\n\n            s_cat = sanitize_filename(category)\n            add_image_if_exists(ws_deep_dive, f\"granular_{s_cat}_negative.png\", f'K{current_row + 2}', OUTPUT_FOLDER_PATH)\n            add_image_if_exists(ws_deep_dive, f\"granular_{s_cat}_positive.png\", f'AB{current_row + 2}', OUTPUT_FOLDER_PATH)\n            add_image_if_exists(ws_deep_dive, f\"granular_{s_cat}_neutral.png\", f'AS{current_row + 2}', OUTPUT_FOLDER_PATH)\n\n            current_row += len(category_view) + 30\n    else:\n        pd.DataFrame([{'Message': \"No granular data available for this report.\"}]).to_excel(writer, sheet_name=sheet_name, index=False)\n\n    # --- Sheet 3: Sentiment Analysis ---\n    print(\"Writing Sheet: Sentiment_Analysis\")\n    row_offset = 0\n    write_df_with_borders(writer, 'Sentiment_Analysis', df_sentiment_by_cat, start_row=row_offset)\n    pos1 = row_offset + len(df_sentiment_by_cat) + 3\n    write_df_with_borders(writer, 'Sentiment_Analysis', df_sentiment_by_subcat, start_row=pos1)\n    ws_sentiment = writer.sheets['Sentiment_Analysis']\n    ws_sentiment.cell(row=row_offset + 1, column=1, value=\"Sentiment Breakdown by Category\")\n    ws_sentiment.cell(row=pos1 + 1, column=1, value=\"Sentiment Breakdown by Sub-Category\")\n    add_image_if_exists(ws_sentiment, 'sentiment_distribution.png', 'G2', OUTPUT_FOLDER_PATH)\n    charts_start_row = pos1 + len(df_sentiment_by_subcat) + 5\n    sentiment_charts = [('Positive Sentiment', 'freq_cat_positive.png', 'freq_subcat_positive.png'), ('Negative Sentiment', 'freq_cat_negative.png', 'freq_subcat_negative.png'), ('Neutral Sentiment', 'freq_cat_neutral.png', 'freq_subcat_neutral.png')]\n    current_row = charts_start_row\n    for title, cat_chart, subcat_chart in sentiment_charts:\n        ws_sentiment.cell(row=current_row, column=1, value=title)\n        add_image_if_exists(ws_sentiment, cat_chart, f'A{current_row + 1}', OUTPUT_FOLDER_PATH)\n        add_image_if_exists(ws_sentiment, subcat_chart, f'K{current_row + 1}', OUTPUT_FOLDER_PATH)\n        current_row += 40\n    \n    # --- Sheet 4: Exploratory Analysis ---\n    print(\"Writing Sheet: Exploratory_Analysis\")\n    row_offset = 0\n    write_df_with_borders(writer, 'Exploratory_Analysis', df_top_words, start_row=row_offset, index=False)\n    pos1 = row_offset + len(df_top_words) + 3\n    write_df_with_borders(writer, 'Exploratory_Analysis', df_top_bigrams, start_row=pos1, index=False)\n    pos2 = pos1 + len(df_top_bigrams) + 3\n    write_df_with_borders(writer, 'Exploratory_Analysis', df_top_trigrams, start_row=pos2, index=False)\n    ws_exploratory = writer.sheets['Exploratory_Analysis']\n    ws_exploratory.cell(row=row_offset + 1, column=1, value=\"Top 20 Words\")\n    ws_exploratory.cell(row=pos1 + 1, column=1, value=\"Top 10 Bigrams\")\n    ws_exploratory.cell(row=pos2 + 1, column=1, value=\"Top 10 Trigrams\")\n    add_image_if_exists(ws_exploratory, 'wordcloud_all.png', 'E2', OUTPUT_FOLDER_PATH)\n    add_image_if_exists(ws_exploratory, 'wordcloud_positive.png', 'E25', OUTPUT_FOLDER_PATH)\n    add_image_if_exists(ws_exploratory, 'wordcloud_negative.png', 'E50', OUTPUT_FOLDER_PATH)\n\n    # --- Sheet 5: Topic Modeling & Uncategorized ---\n    print(\"Writing Sheet: Topic_Modeling_Deep_Dive\")\n    row_offset = 0\n    write_df_with_borders(writer, 'Topic_Modeling_Deep_Dive', df_topics, start_row=row_offset, index=False)\n    pos1 = row_offset + len(df_topics) + 3\n    write_df_with_borders(writer, 'Topic_Modeling_Deep_Dive', df_uncategorized_keywords, start_row=pos1, index=False)\n    ws_topics = writer.sheets['Topic_Modeling_Deep_Dive']\n    ws_topics.cell(row=row_offset + 1, column=1, value=\"Discovered Topics (LDA)\")\n    ws_topics.cell(row=pos1 + 1, column=1, value=\"Top Keywords in Uncategorized Verbatims\")\n\n    # --- Sheet 6: Similarity Analysis ---\n    print(\"Writing Sheet: Similarity_Analysis\")\n    write_df_with_borders(writer, 'Similarity_Analysis', similarity_df, index=False)\n    ws_similarity = writer.sheets['Similarity_Analysis']\n    ws_similarity.cell(row=1, column=1, value=\"Jaccard Similarity Between Sentiments\")\n    current_row = len(similarity_df) + 5\n    ws_similarity.cell(row=current_row, column=1, value=\"Semantic Similarity of Top 20 Words\")\n    add_image_if_exists(ws_similarity, 'heatmap_words.png', f'A{current_row + 1}', OUTPUT_FOLDER_PATH)\n    ws_similarity.cell(row=current_row, column=15, value=\"Jaccard Similarity of Top 10 Bigrams\")\n    add_image_if_exists(ws_similarity, 'heatmap_bigrams.png', f'O{current_row + 1}', OUTPUT_FOLDER_PATH)\n    current_row += 55\n    ws_similarity.cell(row=current_row, column=1, value=\"Jaccard Similarity of Top 10 Trigrams\")\n    add_image_if_exists(ws_similarity, 'heatmap_trigrams.png', f'A{current_row + 1}', OUTPUT_FOLDER_PATH)\n    ws_similarity.cell(row=current_row, column=15, value=\"Jaccard Similarity: Top Words vs. Top Bigrams\")\n    add_image_if_exists(ws_similarity, 'heatmap_words_vs_bigrams.png', f'O{current_row + 1}', OUTPUT_FOLDER_PATH)\n    current_row += 55\n    ws_similarity.cell(row=current_row, column=1, value=\"Jaccard Similarity: Top Words vs. Top Trigrams\")\n    add_image_if_exists(ws_similarity, 'heatmap_words_vs_trigrams.png', f'A{current_row + 1}', OUTPUT_FOLDER_PATH)\n\n# --- Sheet 7: Score Driver Analysis (Overall, Categories, and Sub-Categories) ---\n    sheet_name = 'Score_Driver_Analysis'\n    print(f\"Writing Sheet: {sheet_name}\")\n\n    # Check if the main DataFrame exists to proceed\n    if 'df_significance_overall' in locals() and not df_significance_overall.empty:\n        \n        # --- Write Table 1: Overall Significance ---\n        current_row = 0\n        write_df_with_borders(writer, sheet_name, df_significance_overall.round(3), start_row=current_row, index=False)\n        ws_drivers = writer.sheets[sheet_name]\n        ws_drivers.cell(row=current_row + 1, column=1, value=f\"Overall Key Driver Analysis for '{SCORE_COLUMN_NAME}'\")\n\n        # --- Write Table 2: Category-Only Significance ---\n        # Calculate the starting row for the next table, adding some padding\n        current_row += len(df_significance_overall) + 4 \n        if 'df_significance_categories' in locals() and not df_significance_categories.empty:\n            write_df_with_borders(writer, sheet_name, df_significance_categories.round(3), start_row=current_row, index=False)\n            ws_drivers.cell(row=current_row + 1, column=1, value=\"Category-Only Significance\")\n\n            # Update current_row for the next table\n            current_row += len(df_significance_categories) + 4\n\n        # --- Write Table 3: Sub-Category-Only Significance ---\n        if 'df_significance_subcategories' in locals() and not df_significance_subcategories.empty:\n            write_df_with_borders(writer, sheet_name, df_significance_subcategories.round(3), start_row=current_row, index=False)\n            ws_drivers.cell(row=current_row + 1, column=1, value=\"Sub-Category-Only Significance\")\n\n        # --- Add Plots and Adjust Column Widths ---\n        add_image_if_exists(ws_drivers, 'impact_matrix_categories.png', 'J2', OUTPUT_FOLDER_PATH)\n        add_image_if_exists(ws_drivers, 'impact_matrix_subcategories.png', 'J60', OUTPUT_FOLDER_PATH)\n        \n        # Auto-adjust column widths for better readability\n        for col in ws_drivers.columns:\n            max_length = 0\n            column = col[0].column_letter\n            for cell in col:\n                try:\n                    if len(str(cell.value)) > max_length:\n                        max_length = len(cell.value)\n                except:\n                    pass\n            adjusted_width = (max_length + 2)\n            ws_drivers.column_dimensions[column].width = adjusted_width  \n    \n    # --- Sheet 8: Categorization Results (Full Data) ---\n    print(\"Writing Sheet: Categorization_Results\")\n    write_df_with_borders(writer, 'Categorization_Results', df, index=False)\n\nprint(f\"\\nâœ… All analysis results and plots have been saved to a multi-sheet file:\")\nprint(f\"   {full_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clear the /Kaggle/Working output directory","metadata":{}},{"cell_type":"code","source":"# # Ctrl + / to un/comment out code while highlighted\n\n# import os\n# import shutil\n\n# # This is the directory you want to clear\n# output_dir = '/kaggle/working/'\n\n# # Loop through everything in the directory\n# for filename in os.listdir(output_dir):\n#     file_path = os.path.join(output_dir, filename)\n#     try:\n#         # If it's a file or link, delete it\n#         if os.path.isfile(file_path) or os.path.islink(file_path):\n#             os.unlink(file_path)\n#         # If it's a directory, delete it and all its contents\n#         elif os.path.isdir(file_path):\n#             shutil.rmtree(file_path)\n#         print(f\"Deleted: {filename}\")\n#     except Exception as e:\n#         print(f'Failed to delete {file_path}. Reason: {e}')\n\n# print(\"\\nâœ… Output directory has been cleared.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}