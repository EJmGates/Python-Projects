{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "# --- 1. Install Libraries (Reliable Method) ---\n",
    "import sys\n",
    "# !{sys.executable} -m pip install pandas openpyxl matplotlib wordcloud seaborn\n",
    "# !{sys.executable} -m pip install --upgrade nltk scikit-learn spacy\n",
    "# !{sys.executable} -m pip install --upgrade Pillow==9.5.0\n",
    "\n",
    "# --- 2. Import Libraries ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# ... (the rest of the imports from your original cell)\n",
    "# ... make sure all original imports are here ...\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist, bigrams, trigrams\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# --- 3. Download NLP Models ---\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True) # Correct\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "\n",
    "# --- 4. 🔴 USER CONFIGURATION 🔴 ---\n",
    "EXCEL_FILE_PATH = r'///.xlsx'\n",
    "TEXT_COLUMN_NAME = 'Verbatim'\n",
    "CUSTOM_STOP_WORDS = {'app', 'product', 'service', 'company'}\n",
    "TOPIC_MODEL_TOPICS = 3\n",
    "output_folder_path = r'///Analysis_Results'\n",
    "program_name = \"Program\"\n",
    "kpis_in_scope = \"KPI\"\n",
    "lobs_in_scope = \"LOB\"\n",
    "\n",
    "print(\"✅ Setup complete. All libraries and models are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6550f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading and Preprocessing\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df = pd.read_excel(EXCEL_FILE_PATH)\n",
    "    print(f\"Successfully loaded {len(df)} rows from '{EXCEL_FILE_PATH}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ Warning: File not found. Loading dummy data for demonstration.\")\n",
    "    df = pd.DataFrame({\n",
    "        TEXT_COLUMN_NAME: [\n",
    "            \"The customer service was excellent! Very helpful and friendly.\",\n",
    "            \"I'm very unhappy with the new update. It's slow and buggy.\",\n",
    "            \"It's okay, but the price is too high for what you get.\",\n",
    "            \"Love the new design! The user interface is so much better.\",\n",
    "            \"The app crashes all the time. Please fix this bug.\",\n",
    "        ]\n",
    "    })\n",
    "df.dropna(subset=[TEXT_COLUMN_NAME], inplace=True)\n",
    "\n",
    "# --- 2. Text Cleaning ---\n",
    "stop_words = set(stopwords.words('english')).union(CUSTOM_STOP_WORDS)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\d\\n]', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text.strip())\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "df['cleaned_text'] = df[TEXT_COLUMN_NAME].apply(clean_text)\n",
    "\n",
    "# --- 3. Sentiment Analysis ---\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment_compound'] = df[TEXT_COLUMN_NAME].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "def categorize_sentiment(compound):\n",
    "    if compound >= 0.05: return 'Positive'\n",
    "    if compound <= -0.05: return 'Negative'\n",
    "    return 'Neutral'\n",
    "\n",
    "df['sentiment_label'] = df['sentiment_compound'].apply(categorize_sentiment)\n",
    "\n",
    "print(\"\\n--- Data Preview with Cleaned Text and Sentiment ---\")\n",
    "display(df[[TEXT_COLUMN_NAME, 'cleaned_text', 'sentiment_label']].head())\n",
    "\n",
    "# --- 4. Plot Sentiment Distribution ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['sentiment_label'].value_counts().plot(kind='bar', color=['green', 'red', 'grey'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.ylabel('Number of Responses')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Overall Frequency Analysis & Word Clouds\n",
    "# --- 1. Prepare Overall Text and Tokens ---\n",
    "all_cleaned_text = \" \".join(df['cleaned_text'])\n",
    "all_tokens = word_tokenize(all_cleaned_text)\n",
    "\n",
    "# --- 2. Frequency Distribution ---\n",
    "fdist = FreqDist(all_tokens)\n",
    "print(\"--- Top 20 Most Common Words ---\")\n",
    "print(fdist.most_common(20))\n",
    "fdist.plot(20, title='Top 20 Most Common Words')\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Word Cloud Visualization ---\n",
    "def generate_wordcloud(text, title):\n",
    "    if not text.strip():\n",
    "        print(f\"Skipping '{title}' word cloud: No text available.\")\n",
    "        return\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate clouds for overall, positive, and negative sentiment\n",
    "generate_wordcloud(all_cleaned_text, 'Word Cloud (All Feedback)')\n",
    "generate_wordcloud(\" \".join(df[df.sentiment_label == 'Positive']['cleaned_text']), 'Word Cloud (Positive Feedback)')\n",
    "generate_wordcloud(\" \".join(df[df.sentiment_label == 'Negative']['cleaned_text']), 'Word Cloud (Negative Feedback)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Topic Modeling (LDA)\n",
    "# --- Topic Modeling using LDA ---\n",
    "print(\"\\n--- Discovering Latent Topics ---\")\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df['cleaned_text'].dropna())\n",
    "\n",
    "if dtm.shape[0] > 1:\n",
    "    lda = LatentDirichletAllocation(n_components=TOPIC_MODEL_TOPICS, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    topic_results = []\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_str = \", \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        topic_results.append([f\"Topic #{topic_idx + 1}\", top_words_str])\n",
    "    \n",
    "    # Create a new DataFrame to hold the results\n",
    "    df_topics = pd.DataFrame(topic_results, columns=['Topic', 'Top_Words'])\n",
    "    print(\"Topic Modeling Results:\")\n",
    "    display(df_topics)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = \" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        print(f\"Topic #{topic_idx + 1}: {top_words}\")\n",
    "else:\n",
    "    print(\"Not enough data to perform topic modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a557d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Jaccard Similarity (Between Sentiments)\n",
    "# --- 1. Jaccard Similarity Analysis ---\n",
    "N_WORDS, N_BIGRAMS, N_TRIGRAMS = 20, 10, 10\n",
    "\n",
    "def get_top_ngrams(tokens, n, N_top):\n",
    "    if n == 1:\n",
    "        ngrams_list = tokens\n",
    "    elif n == 2:\n",
    "        ngrams_list = list(bigrams(tokens))\n",
    "    elif n == 3:\n",
    "        ngrams_list = list(trigrams(tokens))\n",
    "    return [item for item, freq in FreqDist(ngrams_list).most_common(N_top)]\n",
    "\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "tokens_by_sentiment = {s: word_tokenize(\" \".join(df[df.sentiment_label == s]['cleaned_text'])) for s in sentiments}\n",
    "\n",
    "top_words = {s: get_top_ngrams(tokens_by_sentiment[s], 1, N_WORDS) for s in sentiments}\n",
    "top_bigrams = {s: get_top_ngrams(tokens_by_sentiment[s], 2, N_BIGRAMS) for s in sentiments}\n",
    "top_trigrams = {s: get_top_ngrams(tokens_by_sentiment[s], 3, N_TRIGRAMS) for s in sentiments}\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "# --- 2. Display Results ---\n",
    "comparisons = [(\"Positive\", \"Negative\"), (\"Positive\", \"Neutral\"), (\"Negative\", \"Neutral\")]\n",
    "results = {\n",
    "    \"Comparison\": [f\"{s1} vs. {s2}\" for s1, s2 in comparisons],\n",
    "    f\"Words (Top {N_WORDS})\": [jaccard_similarity(top_words[s1], top_words[s2]) for s1, s2 in comparisons],\n",
    "    f\"Bigrams (Top {N_BIGRAMS})\": [jaccard_similarity(top_bigrams[s1], top_bigrams[s2]) for s1, s2 in comparisons],\n",
    "    f\"Trigrams (Top {N_TRIGRAMS})\": [jaccard_similarity(top_trigrams[s1], top_trigrams[s2]) for s1, s2 in comparisons]\n",
    "}\n",
    "\n",
    "print(\"\\n--- Vocabulary Similarity Between Sentiments ---\")\n",
    "display(pd.DataFrame(results).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load spaCy Model and Define Jaccard Function ---\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    print(\"\\n--- Calculating Inter-Item Similarity ---\")\n",
    "except OSError:\n",
    "    print(\"spaCy model not found. Please run Cell 1 to download.\")\n",
    "    nlp = None\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "if nlp:\n",
    "    # --- 2. Semantic Similarity for Words ---\n",
    "    top_words_overall = get_top_ngrams(all_tokens, 1, 20)\n",
    "    \n",
    "    print(f\"\\n--- Top {len(top_words_overall)} Overall Words ---\")\n",
    "    print(top_words_overall)\n",
    "\n",
    "    matrix = np.array([[nlp(w1).similarity(nlp(w2)) for w2 in top_words_overall] for w1 in top_words_overall])\n",
    "    similarity_df_words = pd.DataFrame(matrix, index=top_words_overall, columns=top_words_overall)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(similarity_df_words, cmap='viridis')\n",
    "    plt.title('Semantic Similarity Matrix of Top 20 Words', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 3. Jaccard Similarity for N-Grams ---\n",
    "    def create_ngram_df(ngrams):\n",
    "        matrix = np.array([[jaccard_similarity(list(g1), list(g2)) for g2 in ngrams] for g1 in ngrams])\n",
    "        labels = [' '.join(gram) for gram in ngrams]\n",
    "        return pd.DataFrame(matrix, index=labels, columns=labels)\n",
    "\n",
    "    top_bigrams_overall = get_top_ngrams(all_tokens, 2, 10)\n",
    "    top_trigrams_overall = get_top_ngrams(all_tokens, 3, 10)\n",
    "    \n",
    "    print(f\"\\n--- Top {len(top_bigrams_overall)} Overall Bigrams ---\")\n",
    "    print([' '.join(bigram) for bigram in top_bigrams_overall])\n",
    "    \n",
    "    print(f\"\\n--- Top {len(top_trigrams_overall)} Overall Trigrams ---\")\n",
    "    print([' '.join(trigram) for trigram in top_trigrams_overall])\n",
    "    \n",
    "    similarity_df_bigrams = create_ngram_df(top_bigrams_overall)\n",
    "    similarity_df_trigrams = create_ngram_df(top_trigrams_overall)\n",
    "\n",
    "    # Plot the bigram heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df_bigrams, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Jaccard Similarity Matrix of Top 10 Bigrams', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the trigram heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df_trigrams, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Jaccard Similarity Matrix of Top 10 Trigrams', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✅ Similarity matrices created and assigned to variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefe196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from openpyxl.drawing.image import Image # Required for adding images\n",
    "\n",
    "# --- 1. Create the output folder if it doesn't exist ---\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# --- 2. Construct the dynamic filename ---\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "dynamic_filename = f\"{current_date}_{program_name}_{kpis_in_scope}_{lobs_in_scope}_Verbatim_Analysis_Results.xlsx\"\n",
    "full_path_filename = os.path.join(output_folder_path, dynamic_filename)\n",
    "\n",
    "# --- 3. Save Heatmap Plots as Image Files ---\n",
    "# Define image paths\n",
    "path_img_words = os.path.join(output_folder_path, 'word_similarity.png')\n",
    "path_img_bigrams = os.path.join(output_folder_path, 'bigram_similarity.png')\n",
    "path_img_trigrams = os.path.join(output_folder_path, 'trigram_similarity.png')\n",
    "\n",
    "# --- 4. Save word similarity plot\n",
    "fig1 = plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df_words, cmap='viridis')\n",
    "plt.title('Semantic Similarity Matrix of Top 20 Words', fontsize=16)\n",
    "plt.savefig(path_img_words, bbox_inches='tight')\n",
    "plt.close(fig1)\n",
    "\n",
    "# --- 5. Save bigram similarity plot\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_df_bigrams, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Jaccard Similarity Matrix of Top 10 Bigrams', fontsize=16)\n",
    "plt.savefig(path_img_bigrams, bbox_inches='tight')\n",
    "plt.close(fig2)\n",
    "\n",
    "# --- 6. Save trigram similarity plot\n",
    "fig3 = plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_df_trigrams, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Jaccard Similarity Matrix of Top 10 Trigrams', fontsize=16)\n",
    "plt.savefig(path_img_trigrams, bbox_inches='tight')\n",
    "plt.close(fig3)\n",
    "\n",
    "# --- 7. Use ExcelWriter to save all data and embed images ---\n",
    "with pd.ExcelWriter(full_path_filename, engine='openpyxl') as writer:\n",
    "    # Save DataFrames to sheets\n",
    "    df[[TEXT_COLUMN_NAME, 'cleaned_text', 'sentiment_label']].to_excel(writer, sheet_name='Sentiment_Analysis', index=False)\n",
    "    if 'similarity_df' in locals(): similarity_df.to_excel(writer, sheet_name='Jaccard_Similarity', index=False)\n",
    "    if 'df_topics' in locals(): df_topics.to_excel(writer, sheet_name='Topic_Modeling_Results', index=False)\n",
    "    \n",
    "    # Save the matrix data first, then we'll add the image\n",
    "    if 'similarity_df_words' in locals(): similarity_df_words.to_excel(writer, sheet_name='Word_Similarity_Matrix')\n",
    "    if 'similarity_df_bigrams' in locals(): similarity_df_bigrams.to_excel(writer, sheet_name='Bigram_Similarity_Matrix')\n",
    "    if 'similarity_df_trigrams' in locals(): similarity_df_trigrams.to_excel(writer, sheet_name='Trigram_Similarity_Matrix')\n",
    "\n",
    "    # --- Embed Images into the Excel Sheets ---\n",
    "    # Get the workbook and worksheet objects\n",
    "    workbook = writer.book\n",
    "    \n",
    "    # Add word similarity image\n",
    "    if os.path.exists(path_img_words):\n",
    "        ws_words = workbook['Word_Similarity_Matrix']\n",
    "        img_words = Image(path_img_words)\n",
    "        ws_words.add_image(img_words, 'W2') # Position the image in cell W2\n",
    "\n",
    "    # Add bigram similarity image\n",
    "    if os.path.exists(path_img_bigrams):\n",
    "        ws_bigrams = workbook['Bigram_Similarity_Matrix']\n",
    "        img_bigrams = Image(path_img_bigrams)\n",
    "        ws_bigrams.add_image(img_bigrams, 'O2') # Position the image in cell O2\n",
    "\n",
    "    # Add trigram similarity image\n",
    "    if os.path.exists(path_img_trigrams):\n",
    "        ws_trigrams = workbook['Trigram_Similarity_Matrix']\n",
    "        img_trigrams = Image(path_img_trigrams)\n",
    "        ws_trigrams.add_image(img_trigrams, 'O2') # Position the image in cell O2\n",
    "\n",
    "print(f\"✅ All analysis results and plots have been saved to a multi-sheet file:\")\n",
    "print(full_path_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f44942-429a-4530-937d-bdd2214b4f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
